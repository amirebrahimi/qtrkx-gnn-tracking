{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf0b88cf-bc31-46a7-b805-747663004efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow --quiet\n",
    "!pip install tensorflow-quantum --quiet\n",
    "!pip install sklearn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b64732-ada9-40f6-a750-b9ef2a91a6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing configs: \n",
      "train_dir: mews/train\n",
      "valid_dir: mews/valid\n",
      "dataset: mu200_1pT\n",
      "log_dir: logs/test_QGNN_mews/run1/\n",
      "run_type: new_run\n",
      "gpu: -1\n",
      "n_files: 100\n",
      "n_valid: 50\n",
      "n_train: 50\n",
      "batch_size: 1\n",
      "lr_c: 0.01\n",
      "n_iters: 3\n",
      "n_epoch: 20\n",
      "TEST_every: 50\n",
      "hid_dim: 4\n",
      "network: QGNN\n",
      "optimizer: Adam\n",
      "loss_func: BinaryCrossentropy\n",
      "n_thread: 4\n",
      "log_verbosity: 2\n",
      "EN_qc: {'PQC_id': '102', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}\n",
      "NN_qc: {'PQC_id': '102', 'IEC_id': 'simple_encoding_y', 'MC_id': 'measure_all', 'n_layers': 3, 'repetitions': 0, 'n_qubits': 4}\n",
      "Log dir: logs/test_QGNN_mews/run1/\n",
      "Training data input dir: mews/train\n",
      "Validation data input dir: mews/train\n",
      "102_PQC(VAN BUREN)\n",
      "                                                      ┌───────────┐                                     ┌────────────┐                                       ┌────────────┐\n",
      "(0, 0): ───Ry(x0)───Ry(theta0)───@─────────────────────X──────────────Rz(theta4)───@─────────────────────X───────────────Rz(theta8)────@──────────────────────X───────────────Rz(theta12)───\n",
      "                                 │                     │                           │                     │                             │                      │\n",
      "(1, 0): ───Ry(x1)───Ry(theta1)───X───@───Rz(theta5)────┼───────────────────────────X───@───Rz(theta9)────┼─────────────────────────────X───@───Rz(theta13)────┼─────────────────────────────\n",
      "                                     │                 │                               │                 │                                 │                  │\n",
      "(2, 0): ───Ry(x2)───Ry(theta2)───────X───@─────────────┼Rz(theta6)─────────────────────X───@─────────────┼Rz(theta10)──────────────────────X───@──────────────┼Rz(theta14)──────────────────\n",
      "                                         │             │                                   │             │                                     │              │\n",
      "(3, 0): ───Ry(x3)───Ry(theta3)───────────X─────────────@──────────────Rz(theta7)───────────X─────────────@───────────────Rz(theta11)───────────X──────────────@───────────────Rz(theta15)───\n",
      "                                                      └───────────┘                                     └────────────┘                                       └────────────┘\n",
      "102_PQC(VAN BUREN)\n",
      "                                                      ┌───────────┐                                     ┌────────────┐                                       ┌────────────┐\n",
      "(0, 0): ───Ry(x0)───Ry(theta0)───@─────────────────────X──────────────Rz(theta4)───@─────────────────────X───────────────Rz(theta8)────@──────────────────────X───────────────Rz(theta12)───\n",
      "                                 │                     │                           │                     │                             │                      │\n",
      "(1, 0): ───Ry(x1)───Ry(theta1)───X───@───Rz(theta5)────┼───────────────────────────X───@───Rz(theta9)────┼─────────────────────────────X───@───Rz(theta13)────┼─────────────────────────────\n",
      "                                     │                 │                               │                 │                                 │                  │\n",
      "(2, 0): ───Ry(x2)───Ry(theta2)───────X───@─────────────┼Rz(theta6)─────────────────────X───@─────────────┼Rz(theta10)──────────────────────X───@──────────────┼Rz(theta14)──────────────────\n",
      "                                         │             │                                   │             │                                     │              │\n",
      "(3, 0): ───Ry(x3)───Ry(theta3)───────────X─────────────@──────────────Rz(theta7)───────────X─────────────@───────────────Rz(theta11)───────────X──────────────@───────────────Rz(theta15)───\n",
      "                                                      └───────────┘                                     └────────────┘                                       └────────────┘\n",
      "Model: \"GNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " InputNet (Dense)            multiple                  16        \n",
      "                                                                 \n",
      " EdgeNet (EdgeNet)           multiple                  81        \n",
      "                                                                 \n",
      " NodeNet (NodeNet)           multiple                  124       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "2022-02-25 18:18:38.390747 Starting testing the valid set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_7        = (2*precision_7*recall_7)/(precision_7+recall_7)\n",
      "2022-02-25 18:19:17.483360: validation Test:  Loss: 0.8887,  AUC: 0.4979, Acc: 23.1219,  Precision: 0.2312 -- Elapsed: 0m39s\n",
      "2022-02-25 18:19:17.483459 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:19:54.906946: training Test:  Loss: 0.8769,  AUC: 0.5097, Acc: 24.0516,  Precision: 0.2405 -- Elapsed: 0m37s\n",
      "2022-02-25 18:19:54.907165: Training is starting!\n",
      "2022-02-25 18:19:56.494148: Epoch: 1, Batch: 1, Loss: 0.8762, Elapsed: 0m1s\n",
      "2022-02-25 18:19:57.691156: Epoch: 1, Batch: 2, Loss: 0.8679, Elapsed: 0m1s\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Adjoint.differentiate_analytic at 0x7f81c3123ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2022-02-25 18:19:59.202678: Epoch: 1, Batch: 3, Loss: 0.7784, Elapsed: 0m1s\n",
      "WARNING:tensorflow:5 out of the last 15 calls to <function Adjoint.differentiate_analytic at 0x7f81c3123ee0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2022-02-25 18:20:01.685528: Epoch: 1, Batch: 4, Loss: 0.7946, Elapsed: 0m2s\n",
      "2022-02-25 18:20:02.740294: Epoch: 1, Batch: 5, Loss: 0.7587, Elapsed: 0m0s\n",
      "2022-02-25 18:20:04.019849: Epoch: 1, Batch: 6, Loss: 0.7310, Elapsed: 0m1s\n",
      "2022-02-25 18:20:06.253105: Epoch: 1, Batch: 7, Loss: 0.7283, Elapsed: 0m2s\n",
      "2022-02-25 18:20:08.510607: Epoch: 1, Batch: 8, Loss: 0.6891, Elapsed: 0m2s\n",
      "2022-02-25 18:20:10.353611: Epoch: 1, Batch: 9, Loss: 0.6779, Elapsed: 0m1s\n",
      "2022-02-25 18:20:12.059068: Epoch: 1, Batch: 10, Loss: 0.6662, Elapsed: 0m1s\n",
      "2022-02-25 18:20:13.398652: Epoch: 1, Batch: 11, Loss: 0.6546, Elapsed: 0m1s\n",
      "2022-02-25 18:20:14.776124: Epoch: 1, Batch: 12, Loss: 0.6463, Elapsed: 0m1s\n",
      "2022-02-25 18:20:16.212065: Epoch: 1, Batch: 13, Loss: 0.6480, Elapsed: 0m1s\n",
      "2022-02-25 18:20:17.335150: Epoch: 1, Batch: 14, Loss: 0.6615, Elapsed: 0m0s\n",
      "2022-02-25 18:20:18.628150: Epoch: 1, Batch: 15, Loss: 0.6478, Elapsed: 0m1s\n",
      "2022-02-25 18:20:19.686617: Epoch: 1, Batch: 16, Loss: 0.6401, Elapsed: 0m0s\n",
      "2022-02-25 18:20:21.091204: Epoch: 1, Batch: 17, Loss: 0.6176, Elapsed: 0m1s\n",
      "2022-02-25 18:20:22.434793: Epoch: 1, Batch: 18, Loss: 0.6010, Elapsed: 0m1s\n",
      "2022-02-25 18:20:24.017434: Epoch: 1, Batch: 19, Loss: 0.5853, Elapsed: 0m1s\n",
      "2022-02-25 18:20:25.249521: Epoch: 1, Batch: 20, Loss: 0.6136, Elapsed: 0m1s\n",
      "2022-02-25 18:20:26.635188: Epoch: 1, Batch: 21, Loss: 0.5981, Elapsed: 0m1s\n",
      "2022-02-25 18:20:28.048331: Epoch: 1, Batch: 22, Loss: 0.5791, Elapsed: 0m1s\n",
      "2022-02-25 18:20:29.515674: Epoch: 1, Batch: 23, Loss: 0.5798, Elapsed: 0m1s\n",
      "2022-02-25 18:20:30.582734: Epoch: 1, Batch: 24, Loss: 0.6062, Elapsed: 0m0s\n",
      "2022-02-25 18:20:31.549443: Epoch: 1, Batch: 25, Loss: 0.6544, Elapsed: 0m0s\n",
      "2022-02-25 18:20:32.885675: Epoch: 1, Batch: 26, Loss: 0.5869, Elapsed: 0m1s\n",
      "2022-02-25 18:20:33.917289: Epoch: 1, Batch: 27, Loss: 0.6394, Elapsed: 0m0s\n",
      "2022-02-25 18:20:35.567311: Epoch: 1, Batch: 28, Loss: 0.5714, Elapsed: 0m1s\n",
      "2022-02-25 18:20:36.865125: Epoch: 1, Batch: 29, Loss: 0.5471, Elapsed: 0m1s\n",
      "2022-02-25 18:20:38.111711: Epoch: 1, Batch: 30, Loss: 0.5940, Elapsed: 0m1s\n",
      "2022-02-25 18:20:40.018537: Epoch: 1, Batch: 31, Loss: 0.5328, Elapsed: 0m1s\n",
      "2022-02-25 18:20:42.290757: Epoch: 1, Batch: 32, Loss: 0.5105, Elapsed: 0m2s\n",
      "2022-02-25 18:20:43.677109: Epoch: 1, Batch: 33, Loss: 0.5701, Elapsed: 0m1s\n",
      "2022-02-25 18:20:45.362624: Epoch: 1, Batch: 34, Loss: 0.5393, Elapsed: 0m1s\n",
      "2022-02-25 18:20:47.291785: Epoch: 1, Batch: 35, Loss: 0.5342, Elapsed: 0m1s\n",
      "2022-02-25 18:20:48.806757: Epoch: 1, Batch: 36, Loss: 0.5681, Elapsed: 0m1s\n",
      "2022-02-25 18:20:49.791031: Epoch: 1, Batch: 37, Loss: 0.6364, Elapsed: 0m0s\n",
      "2022-02-25 18:20:51.515352: Epoch: 1, Batch: 38, Loss: 0.5435, Elapsed: 0m1s\n",
      "2022-02-25 18:20:52.888073: Epoch: 1, Batch: 39, Loss: 0.5841, Elapsed: 0m1s\n",
      "2022-02-25 18:20:55.126904: Epoch: 1, Batch: 40, Loss: 0.4906, Elapsed: 0m2s\n",
      "2022-02-25 18:20:57.018393: Epoch: 1, Batch: 41, Loss: 0.5448, Elapsed: 0m1s\n",
      "2022-02-25 18:20:58.531647: Epoch: 1, Batch: 42, Loss: 0.5549, Elapsed: 0m1s\n",
      "2022-02-25 18:20:59.875048: Epoch: 1, Batch: 43, Loss: 0.5815, Elapsed: 0m1s\n",
      "2022-02-25 18:21:01.819657: Epoch: 1, Batch: 44, Loss: 0.5267, Elapsed: 0m1s\n",
      "2022-02-25 18:21:03.201163: Epoch: 1, Batch: 45, Loss: 0.5950, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:21:04.887581: Epoch: 1, Batch: 46, Loss: 0.5239, Elapsed: 0m1s\n",
      "2022-02-25 18:21:06.399545: Epoch: 1, Batch: 47, Loss: 0.5509, Elapsed: 0m1s\n",
      "2022-02-25 18:21:08.073843: Epoch: 1, Batch: 48, Loss: 0.5374, Elapsed: 0m1s\n",
      "2022-02-25 18:21:10.095100: Epoch: 1, Batch: 49, Loss: 0.5203, Elapsed: 0m1s\n",
      "2022-02-25 18:21:11.264673: Epoch: 1, Batch: 50, Loss: 0.5961, Elapsed: 0m0s\n",
      "2022-02-25 18:21:11.539236 Starting testing the valid set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:83: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_5 = tp/(tp+fp) # also named purity\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:21:50.885175: validation Test:  Loss: 0.5365,  AUC: 0.5207, Acc: 76.8781,  Precision: nan -- Elapsed: 0m39s\n",
      "2022-02-25 18:21:50.885252 Starting testing the train set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:83: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_5 = tp/(tp+fp) # also named purity\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:22:28.453945: training Test:  Loss: 0.5476,  AUC: 0.5163, Acc: 75.9484,  Precision: nan -- Elapsed: 0m37s\n",
      "2022-02-25 18:22:29.553006: Epoch: 2, Batch: 1, Loss: 0.5349, Elapsed: 0m1s\n",
      "2022-02-25 18:22:31.813123: Epoch: 2, Batch: 2, Loss: 0.4981, Elapsed: 0m2s\n",
      "2022-02-25 18:22:33.257445: Epoch: 2, Batch: 3, Loss: 0.5530, Elapsed: 0m1s\n",
      "2022-02-25 18:22:34.419810: Epoch: 2, Batch: 4, Loss: 0.5928, Elapsed: 0m0s\n",
      "2022-02-25 18:22:35.793082: Epoch: 2, Batch: 5, Loss: 0.5560, Elapsed: 0m1s\n",
      "2022-02-25 18:22:37.482935: Epoch: 2, Batch: 6, Loss: 0.5381, Elapsed: 0m1s\n",
      "2022-02-25 18:22:38.845320: Epoch: 2, Batch: 7, Loss: 0.5683, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:22:40.570915: Epoch: 2, Batch: 8, Loss: 0.5405, Elapsed: 0m1s\n",
      "2022-02-25 18:22:42.239393: Epoch: 2, Batch: 9, Loss: 0.5313, Elapsed: 0m1s\n",
      "2022-02-25 18:22:43.193088: Epoch: 2, Batch: 10, Loss: 0.6466, Elapsed: 0m0s\n",
      "2022-02-25 18:22:44.310292: Epoch: 2, Batch: 11, Loss: 0.6258, Elapsed: 0m0s\n",
      "2022-02-25 18:22:45.871779: Epoch: 2, Batch: 12, Loss: 0.5233, Elapsed: 0m1s\n",
      "2022-02-25 18:22:47.740107: Epoch: 2, Batch: 13, Loss: 0.5244, Elapsed: 0m1s\n",
      "2022-02-25 18:22:49.155042: Epoch: 2, Batch: 14, Loss: 0.5819, Elapsed: 0m1s\n",
      "2022-02-25 18:22:51.005933: Epoch: 2, Batch: 15, Loss: 0.5154, Elapsed: 0m1s\n",
      "2022-02-25 18:22:52.638796: Epoch: 2, Batch: 16, Loss: 0.5568, Elapsed: 0m1s\n",
      "2022-02-25 18:22:54.744793: Epoch: 2, Batch: 17, Loss: 0.4824, Elapsed: 0m1s\n",
      "2022-02-25 18:22:55.789489: Epoch: 2, Batch: 18, Loss: 0.6024, Elapsed: 0m0s\n",
      "2022-02-25 18:22:57.192780: Epoch: 2, Batch: 19, Loss: 0.5663, Elapsed: 0m1s\n",
      "2022-02-25 18:22:58.219704: Epoch: 2, Batch: 20, Loss: 0.6501, Elapsed: 0m0s\n",
      "2022-02-25 18:22:59.670489: Epoch: 2, Batch: 21, Loss: 0.5657, Elapsed: 0m1s\n",
      "2022-02-25 18:23:00.881144: Epoch: 2, Batch: 22, Loss: 0.5933, Elapsed: 0m1s\n",
      "2022-02-25 18:23:02.768632: Epoch: 2, Batch: 23, Loss: 0.5246, Elapsed: 0m1s\n",
      "2022-02-25 18:23:03.992765: Epoch: 2, Batch: 24, Loss: 0.6133, Elapsed: 0m1s\n",
      "2022-02-25 18:23:06.210278: Epoch: 2, Batch: 25, Loss: 0.4743, Elapsed: 0m2s\n",
      "2022-02-25 18:23:07.592220: Epoch: 2, Batch: 26, Loss: 0.5848, Elapsed: 0m1s\n",
      "2022-02-25 18:23:09.135581: Epoch: 2, Batch: 27, Loss: 0.5505, Elapsed: 0m1s\n",
      "2022-02-25 18:23:10.545577: Epoch: 2, Batch: 28, Loss: 0.5624, Elapsed: 0m1s\n",
      "2022-02-25 18:23:11.740109: Epoch: 2, Batch: 29, Loss: 0.5747, Elapsed: 0m1s\n",
      "2022-02-25 18:23:14.036967: Epoch: 2, Batch: 30, Loss: 0.5109, Elapsed: 0m2s\n",
      "2022-02-25 18:23:15.136774: Epoch: 2, Batch: 31, Loss: 0.6047, Elapsed: 0m0s\n",
      "2022-02-25 18:23:16.101785: Epoch: 2, Batch: 32, Loss: 0.6720, Elapsed: 0m0s\n",
      "2022-02-25 18:23:17.639847: Epoch: 2, Batch: 33, Loss: 0.5335, Elapsed: 0m1s\n",
      "2022-02-25 18:23:19.218940: Epoch: 2, Batch: 34, Loss: 0.5603, Elapsed: 0m1s\n",
      "2022-02-25 18:23:20.502560: Epoch: 2, Batch: 35, Loss: 0.5902, Elapsed: 0m1s\n",
      "2022-02-25 18:23:21.964856: Epoch: 2, Batch: 36, Loss: 0.5225, Elapsed: 0m1s\n",
      "2022-02-25 18:23:23.008009: Epoch: 2, Batch: 37, Loss: 0.6068, Elapsed: 0m0s\n",
      "2022-02-25 18:23:24.814384: Epoch: 2, Batch: 38, Loss: 0.5427, Elapsed: 0m1s\n",
      "2022-02-25 18:23:26.798590: Epoch: 2, Batch: 39, Loss: 0.5203, Elapsed: 0m1s\n",
      "2022-02-25 18:23:28.219933: Epoch: 2, Batch: 40, Loss: 0.5606, Elapsed: 0m1s\n",
      "2022-02-25 18:23:29.650081: Epoch: 2, Batch: 41, Loss: 0.5745, Elapsed: 0m1s\n",
      "2022-02-25 18:23:31.501993: Epoch: 2, Batch: 42, Loss: 0.5334, Elapsed: 0m1s\n",
      "2022-02-25 18:23:33.089320: Epoch: 2, Batch: 43, Loss: 0.5271, Elapsed: 0m1s\n",
      "2022-02-25 18:23:34.233397: Epoch: 2, Batch: 44, Loss: 0.5936, Elapsed: 0m0s\n",
      "2022-02-25 18:23:35.589113: Epoch: 2, Batch: 45, Loss: 0.5925, Elapsed: 0m1s\n",
      "2022-02-25 18:23:36.786385: Epoch: 2, Batch: 46, Loss: 0.5896, Elapsed: 0m1s\n",
      "2022-02-25 18:23:38.492453: Epoch: 2, Batch: 47, Loss: 0.5223, Elapsed: 0m1s\n",
      "2022-02-25 18:23:39.927832: Epoch: 2, Batch: 48, Loss: 0.5876, Elapsed: 0m1s\n",
      "2022-02-25 18:23:42.401426: Epoch: 2, Batch: 49, Loss: 0.4921, Elapsed: 0m2s\n",
      "2022-02-25 18:23:43.825115: Epoch: 2, Batch: 50, Loss: 0.5389, Elapsed: 0m1s\n",
      "2022-02-25 18:23:44.030287 Starting testing the valid set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:83: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_5 = tp/(tp+fp) # also named purity\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:24:23.173271: validation Test:  Loss: 0.5356,  AUC: 0.5356, Acc: 76.8781,  Precision: nan -- Elapsed: 0m39s\n",
      "2022-02-25 18:24:23.173346 Starting testing the train set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:83: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_5 = tp/(tp+fp) # also named purity\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:25:00.823544: training Test:  Loss: 0.5457,  AUC: 0.5349, Acc: 75.9484,  Precision: nan -- Elapsed: 0m37s\n",
      "2022-02-25 18:25:01.668380: Epoch: 3, Batch: 1, Loss: 0.5976, Elapsed: 0m0s\n",
      "2022-02-25 18:25:02.782000: Epoch: 3, Batch: 2, Loss: 0.6199, Elapsed: 0m0s\n",
      "2022-02-25 18:25:04.435260: Epoch: 3, Batch: 3, Loss: 0.5233, Elapsed: 0m1s\n",
      "2022-02-25 18:25:06.085188: Epoch: 3, Batch: 4, Loss: 0.5576, Elapsed: 0m1s\n",
      "2022-02-25 18:25:07.305637: Epoch: 3, Batch: 5, Loss: 0.5861, Elapsed: 0m1s\n",
      "2022-02-25 18:25:08.693063: Epoch: 3, Batch: 6, Loss: 0.5891, Elapsed: 0m1s\n",
      "2022-02-25 18:25:09.703612: Epoch: 3, Batch: 7, Loss: 0.6609, Elapsed: 0m0s\n",
      "2022-02-25 18:25:11.600580: Epoch: 3, Batch: 8, Loss: 0.5163, Elapsed: 0m1s\n",
      "2022-02-25 18:25:13.228668: Epoch: 3, Batch: 9, Loss: 0.5377, Elapsed: 0m1s\n",
      "2022-02-25 18:25:14.615870: Epoch: 3, Batch: 10, Loss: 0.5831, Elapsed: 0m1s\n",
      "2022-02-25 18:25:16.194240: Epoch: 3, Batch: 11, Loss: 0.5333, Elapsed: 0m1s\n",
      "2022-02-25 18:25:17.537959: Epoch: 3, Batch: 12, Loss: 0.5315, Elapsed: 0m1s\n",
      "2022-02-25 18:25:19.012784: Epoch: 3, Batch: 13, Loss: 0.5269, Elapsed: 0m1s\n",
      "2022-02-25 18:25:20.420486: Epoch: 3, Batch: 14, Loss: 0.5390, Elapsed: 0m1s\n",
      "2022-02-25 18:25:22.183386: Epoch: 3, Batch: 15, Loss: 0.5221, Elapsed: 0m1s\n",
      "2022-02-25 18:25:23.805526: Epoch: 3, Batch: 16, Loss: 0.5580, Elapsed: 0m1s\n",
      "2022-02-25 18:25:25.241711: Epoch: 3, Batch: 17, Loss: 0.5582, Elapsed: 0m1s\n",
      "2022-02-25 18:25:26.733514: Epoch: 3, Batch: 18, Loss: 0.5871, Elapsed: 0m1s\n",
      "2022-02-25 18:25:28.132076: Epoch: 3, Batch: 19, Loss: 0.5770, Elapsed: 0m1s\n",
      "2022-02-25 18:25:30.016839: Epoch: 3, Batch: 20, Loss: 0.5252, Elapsed: 0m1s\n",
      "2022-02-25 18:25:31.478801: Epoch: 3, Batch: 21, Loss: 0.5613, Elapsed: 0m1s\n",
      "2022-02-25 18:25:32.943142: Epoch: 3, Batch: 22, Loss: 0.5531, Elapsed: 0m1s\n",
      "2022-02-25 18:25:35.403512: Epoch: 3, Batch: 23, Loss: 0.4928, Elapsed: 0m2s\n",
      "2022-02-25 18:25:36.832674: Epoch: 3, Batch: 24, Loss: 0.5626, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:25:37.828039: Epoch: 3, Batch: 25, Loss: 0.6361, Elapsed: 0m0s\n",
      "2022-02-25 18:25:39.126377: Epoch: 3, Batch: 26, Loss: 0.5697, Elapsed: 0m1s\n",
      "2022-02-25 18:25:40.408137: Epoch: 3, Batch: 27, Loss: 0.5851, Elapsed: 0m1s\n",
      "2022-02-25 18:25:42.011558: Epoch: 3, Batch: 28, Loss: 0.5294, Elapsed: 0m1s\n",
      "2022-02-25 18:25:43.471519: Epoch: 3, Batch: 29, Loss: 0.5599, Elapsed: 0m1s\n",
      "2022-02-25 18:25:45.636584: Epoch: 3, Batch: 30, Loss: 0.4714, Elapsed: 0m1s\n",
      "2022-02-25 18:25:46.784667: Epoch: 3, Batch: 31, Loss: 0.5937, Elapsed: 0m0s\n",
      "2022-02-25 18:25:49.078318: Epoch: 3, Batch: 32, Loss: 0.5115, Elapsed: 0m2s\n",
      "2022-02-25 18:25:50.566317: Epoch: 3, Batch: 33, Loss: 0.5481, Elapsed: 0m1s\n",
      "2022-02-25 18:25:51.749716: Epoch: 3, Batch: 34, Loss: 0.5856, Elapsed: 0m0s\n",
      "2022-02-25 18:25:53.874461: Epoch: 3, Batch: 35, Loss: 0.4846, Elapsed: 0m1s\n",
      "2022-02-25 18:25:55.152015: Epoch: 3, Batch: 36, Loss: 0.6051, Elapsed: 0m1s\n",
      "2022-02-25 18:25:57.475416: Epoch: 3, Batch: 37, Loss: 0.4871, Elapsed: 0m2s\n",
      "2022-02-25 18:25:58.736871: Epoch: 3, Batch: 38, Loss: 0.5867, Elapsed: 0m1s\n",
      "2022-02-25 18:26:00.380454: Epoch: 3, Batch: 39, Loss: 0.5461, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:26:02.280636: Epoch: 3, Batch: 40, Loss: 0.5212, Elapsed: 0m1s\n",
      "2022-02-25 18:26:03.634221: Epoch: 3, Batch: 41, Loss: 0.5734, Elapsed: 0m1s\n",
      "2022-02-25 18:26:05.293744: Epoch: 3, Batch: 42, Loss: 0.5388, Elapsed: 0m1s\n",
      "2022-02-25 18:26:06.620573: Epoch: 3, Batch: 43, Loss: 0.5747, Elapsed: 0m1s\n",
      "2022-02-25 18:26:08.306598: Epoch: 3, Batch: 44, Loss: 0.5249, Elapsed: 0m1s\n",
      "2022-02-25 18:26:09.349406: Epoch: 3, Batch: 45, Loss: 0.6277, Elapsed: 0m0s\n",
      "2022-02-25 18:26:10.562443: Epoch: 3, Batch: 46, Loss: 0.5989, Elapsed: 0m1s\n",
      "2022-02-25 18:26:12.557376: Epoch: 3, Batch: 47, Loss: 0.5151, Elapsed: 0m1s\n",
      "2022-02-25 18:26:14.360555: Epoch: 3, Batch: 48, Loss: 0.5388, Elapsed: 0m1s\n",
      "2022-02-25 18:26:15.761400: Epoch: 3, Batch: 49, Loss: 0.5669, Elapsed: 0m1s\n",
      "2022-02-25 18:26:17.593984: Epoch: 3, Batch: 50, Loss: 0.5316, Elapsed: 0m1s\n",
      "2022-02-25 18:26:17.808828 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:26:57.148282: validation Test:  Loss: 0.5313,  AUC: 0.5598, Acc: 77.1156,  Precision: 0.6042 -- Elapsed: 0m39s\n",
      "2022-02-25 18:26:57.148346 Starting testing the train set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:93: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  f1_7        = (2*precision_7*recall_7)/(precision_7+recall_7)\n",
      "2022-02-25 18:27:34.547424: training Test:  Loss: 0.5409,  AUC: 0.5658, Acc: 76.2347,  Precision: 0.6252 -- Elapsed: 0m37s\n",
      "2022-02-25 18:27:35.793078: Epoch: 4, Batch: 1, Loss: 0.5832, Elapsed: 0m1s\n",
      "2022-02-25 18:27:37.866231: Epoch: 4, Batch: 2, Loss: 0.5158, Elapsed: 0m1s\n",
      "2022-02-25 18:27:39.235620: Epoch: 4, Batch: 3, Loss: 0.5792, Elapsed: 0m1s\n",
      "2022-02-25 18:27:41.389493: Epoch: 4, Batch: 4, Loss: 0.4860, Elapsed: 0m1s\n",
      "2022-02-25 18:27:43.861968: Epoch: 4, Batch: 5, Loss: 0.4902, Elapsed: 0m2s\n",
      "2022-02-25 18:27:44.933718: Epoch: 4, Batch: 6, Loss: 0.5877, Elapsed: 0m0s\n",
      "2022-02-25 18:27:46.336840: Epoch: 4, Batch: 7, Loss: 0.5447, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:27:47.757221: Epoch: 4, Batch: 8, Loss: 0.5522, Elapsed: 0m1s\n",
      "2022-02-25 18:27:49.162977: Epoch: 4, Batch: 9, Loss: 0.5751, Elapsed: 0m1s\n",
      "2022-02-25 18:27:50.574397: Epoch: 4, Batch: 10, Loss: 0.5642, Elapsed: 0m1s\n",
      "2022-02-25 18:27:51.743672: Epoch: 4, Batch: 11, Loss: 0.5865, Elapsed: 0m0s\n",
      "2022-02-25 18:27:53.364266: Epoch: 4, Batch: 12, Loss: 0.5313, Elapsed: 0m1s\n",
      "2022-02-25 18:27:54.688362: Epoch: 4, Batch: 13, Loss: 0.5160, Elapsed: 0m1s\n",
      "2022-02-25 18:27:56.839542: Epoch: 4, Batch: 14, Loss: 0.4592, Elapsed: 0m1s\n",
      "2022-02-25 18:27:58.462461: Epoch: 4, Batch: 15, Loss: 0.5452, Elapsed: 0m1s\n",
      "2022-02-25 18:27:59.926343: Epoch: 4, Batch: 16, Loss: 0.5457, Elapsed: 0m1s\n",
      "2022-02-25 18:28:01.349969: Epoch: 4, Batch: 17, Loss: 0.5568, Elapsed: 0m1s\n",
      "2022-02-25 18:28:02.793754: Epoch: 4, Batch: 18, Loss: 0.5389, Elapsed: 0m1s\n",
      "2022-02-25 18:28:03.789767: Epoch: 4, Batch: 19, Loss: 0.6135, Elapsed: 0m0s\n",
      "2022-02-25 18:28:05.372906: Epoch: 4, Batch: 20, Loss: 0.5237, Elapsed: 0m1s\n",
      "2022-02-25 18:28:06.613695: Epoch: 4, Batch: 21, Loss: 0.5646, Elapsed: 0m1s\n",
      "2022-02-25 18:28:07.632377: Epoch: 4, Batch: 22, Loss: 0.6255, Elapsed: 0m0s\n",
      "2022-02-25 18:28:08.910035: Epoch: 4, Batch: 23, Loss: 0.5612, Elapsed: 0m1s\n",
      "2022-02-25 18:28:10.577782: Epoch: 4, Batch: 24, Loss: 0.5456, Elapsed: 0m1s\n",
      "2022-02-25 18:28:12.540405: Epoch: 4, Batch: 25, Loss: 0.5177, Elapsed: 0m1s\n",
      "2022-02-25 18:28:13.787590: Epoch: 4, Batch: 26, Loss: 0.5596, Elapsed: 0m1s\n",
      "2022-02-25 18:28:14.762968: Epoch: 4, Batch: 27, Loss: 0.5991, Elapsed: 0m0s\n",
      "2022-02-25 18:28:16.653398: Epoch: 4, Batch: 28, Loss: 0.5015, Elapsed: 0m1s\n",
      "2022-02-25 18:28:18.339893: Epoch: 4, Batch: 29, Loss: 0.5150, Elapsed: 0m1s\n",
      "2022-02-25 18:28:19.715411: Epoch: 4, Batch: 30, Loss: 0.5603, Elapsed: 0m1s\n",
      "2022-02-25 18:28:21.633652: Epoch: 4, Batch: 31, Loss: 0.5260, Elapsed: 0m1s\n",
      "2022-02-25 18:28:23.369864: Epoch: 4, Batch: 32, Loss: 0.5359, Elapsed: 0m1s\n",
      "2022-02-25 18:28:24.937552: Epoch: 4, Batch: 33, Loss: 0.5248, Elapsed: 0m1s\n",
      "2022-02-25 18:28:26.389435: Epoch: 4, Batch: 34, Loss: 0.5049, Elapsed: 0m1s\n",
      "2022-02-25 18:28:27.701882: Epoch: 4, Batch: 35, Loss: 0.6004, Elapsed: 0m1s\n",
      "2022-02-25 18:28:28.947252: Epoch: 4, Batch: 36, Loss: 0.5726, Elapsed: 0m1s\n",
      "2022-02-25 18:28:30.717587: Epoch: 4, Batch: 37, Loss: 0.5046, Elapsed: 0m1s\n",
      "2022-02-25 18:28:32.030384: Epoch: 4, Batch: 38, Loss: 0.5823, Elapsed: 0m1s\n",
      "2022-02-25 18:28:33.624895: Epoch: 4, Batch: 39, Loss: 0.5132, Elapsed: 0m1s\n",
      "2022-02-25 18:28:34.718600: Epoch: 4, Batch: 40, Loss: 0.5790, Elapsed: 0m0s\n",
      "2022-02-25 18:28:36.099647: Epoch: 4, Batch: 41, Loss: 0.5593, Elapsed: 0m1s\n",
      "2022-02-25 18:28:37.629487: Epoch: 4, Batch: 42, Loss: 0.5753, Elapsed: 0m1s\n",
      "2022-02-25 18:28:38.952886: Epoch: 4, Batch: 43, Loss: 0.5462, Elapsed: 0m1s\n",
      "2022-02-25 18:28:41.229673: Epoch: 4, Batch: 44, Loss: 0.5014, Elapsed: 0m2s\n",
      "2022-02-25 18:28:42.721562: Epoch: 4, Batch: 45, Loss: 0.5413, Elapsed: 0m1s\n",
      "2022-02-25 18:28:44.432210: Epoch: 4, Batch: 46, Loss: 0.5445, Elapsed: 0m1s\n",
      "2022-02-25 18:28:46.332273: Epoch: 4, Batch: 47, Loss: 0.5298, Elapsed: 0m1s\n",
      "2022-02-25 18:28:48.601506: Epoch: 4, Batch: 48, Loss: 0.4775, Elapsed: 0m2s\n",
      "2022-02-25 18:28:50.477527: Epoch: 4, Batch: 49, Loss: 0.5063, Elapsed: 0m1s\n",
      "2022-02-25 18:28:52.107284: Epoch: 4, Batch: 50, Loss: 0.5284, Elapsed: 0m1s\n",
      "2022-02-25 18:28:52.323317 Starting testing the valid set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:29:31.520704: validation Test:  Loss: 0.5237,  AUC: 0.6347, Acc: 76.5159,  Precision: 0.4753 -- Elapsed: 0m39s\n",
      "2022-02-25 18:29:31.520782 Starting testing the train set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:30:09.136726: training Test:  Loss: 0.5352,  AUC: 0.6282, Acc: 75.6585,  Precision: 0.4804 -- Elapsed: 0m37s\n",
      "2022-02-25 18:30:10.921646: Epoch: 5, Batch: 1, Loss: 0.5056, Elapsed: 0m1s\n",
      "2022-02-25 18:30:12.005304: Epoch: 5, Batch: 2, Loss: 0.5805, Elapsed: 0m0s\n",
      "2022-02-25 18:30:13.757350: Epoch: 5, Batch: 3, Loss: 0.4994, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:30:15.394028: Epoch: 5, Batch: 4, Loss: 0.5120, Elapsed: 0m1s\n",
      "2022-02-25 18:30:16.654618: Epoch: 5, Batch: 5, Loss: 0.5517, Elapsed: 0m1s\n",
      "2022-02-25 18:30:18.271133: Epoch: 5, Batch: 6, Loss: 0.5166, Elapsed: 0m1s\n",
      "2022-02-25 18:30:19.797641: Epoch: 5, Batch: 7, Loss: 0.5462, Elapsed: 0m1s\n",
      "2022-02-25 18:30:20.811205: Epoch: 5, Batch: 8, Loss: 0.6441, Elapsed: 0m0s\n",
      "2022-02-25 18:30:22.213548: Epoch: 5, Batch: 9, Loss: 0.5502, Elapsed: 0m1s\n",
      "2022-02-25 18:30:24.132680: Epoch: 5, Batch: 10, Loss: 0.5138, Elapsed: 0m1s\n",
      "2022-02-25 18:30:25.503385: Epoch: 5, Batch: 11, Loss: 0.5575, Elapsed: 0m1s\n",
      "2022-02-25 18:30:26.860112: Epoch: 5, Batch: 12, Loss: 0.5779, Elapsed: 0m1s\n",
      "2022-02-25 18:30:28.270103: Epoch: 5, Batch: 13, Loss: 0.5065, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:30:29.420666: Epoch: 5, Batch: 14, Loss: 0.5803, Elapsed: 0m0s\n",
      "2022-02-25 18:30:31.267441: Epoch: 5, Batch: 15, Loss: 0.4982, Elapsed: 0m1s\n",
      "2022-02-25 18:30:32.628118: Epoch: 5, Batch: 16, Loss: 0.5439, Elapsed: 0m1s\n",
      "2022-02-25 18:30:34.299613: Epoch: 5, Batch: 17, Loss: 0.5142, Elapsed: 0m1s\n",
      "2022-02-25 18:30:35.967048: Epoch: 5, Batch: 18, Loss: 0.5428, Elapsed: 0m1s\n",
      "2022-02-25 18:30:37.562118: Epoch: 5, Batch: 19, Loss: 0.5104, Elapsed: 0m1s\n",
      "2022-02-25 18:30:38.661915: Epoch: 5, Batch: 20, Loss: 0.5763, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:30:40.498752: Epoch: 5, Batch: 21, Loss: 0.5297, Elapsed: 0m1s\n",
      "2022-02-25 18:30:41.867924: Epoch: 5, Batch: 22, Loss: 0.5602, Elapsed: 0m1s\n",
      "2022-02-25 18:30:43.233123: Epoch: 5, Batch: 23, Loss: 0.5058, Elapsed: 0m1s\n",
      "2022-02-25 18:30:45.103959: Epoch: 5, Batch: 24, Loss: 0.5218, Elapsed: 0m1s\n",
      "2022-02-25 18:30:46.587820: Epoch: 5, Batch: 25, Loss: 0.5731, Elapsed: 0m1s\n",
      "2022-02-25 18:30:47.658423: Epoch: 5, Batch: 26, Loss: 0.5910, Elapsed: 0m0s\n",
      "2022-02-25 18:30:49.138575: Epoch: 5, Batch: 27, Loss: 0.5378, Elapsed: 0m1s\n",
      "2022-02-25 18:30:51.420347: Epoch: 5, Batch: 28, Loss: 0.4385, Elapsed: 0m2s\n",
      "2022-02-25 18:30:53.677786: Epoch: 5, Batch: 29, Loss: 0.4682, Elapsed: 0m2s\n",
      "2022-02-25 18:30:55.064408: Epoch: 5, Batch: 30, Loss: 0.5575, Elapsed: 0m1s\n",
      "2022-02-25 18:30:57.134980: Epoch: 5, Batch: 31, Loss: 0.5012, Elapsed: 0m1s\n",
      "2022-02-25 18:30:58.856015: Epoch: 5, Batch: 32, Loss: 0.5322, Elapsed: 0m1s\n",
      "2022-02-25 18:30:59.873533: Epoch: 5, Batch: 33, Loss: 0.5924, Elapsed: 0m0s\n",
      "2022-02-25 18:31:01.602583: Epoch: 5, Batch: 34, Loss: 0.5400, Elapsed: 0m1s\n",
      "2022-02-25 18:31:03.023043: Epoch: 5, Batch: 35, Loss: 0.5611, Elapsed: 0m1s\n",
      "2022-02-25 18:31:04.529154: Epoch: 5, Batch: 36, Loss: 0.5289, Elapsed: 0m1s\n",
      "2022-02-25 18:31:06.998844: Epoch: 5, Batch: 37, Loss: 0.4733, Elapsed: 0m2s\n",
      "2022-02-25 18:31:09.228702: Epoch: 5, Batch: 38, Loss: 0.4714, Elapsed: 0m2s\n",
      "2022-02-25 18:31:10.468542: Epoch: 5, Batch: 39, Loss: 0.5600, Elapsed: 0m1s\n",
      "2022-02-25 18:31:11.922884: Epoch: 5, Batch: 40, Loss: 0.5469, Elapsed: 0m1s\n",
      "2022-02-25 18:31:13.191288: Epoch: 5, Batch: 41, Loss: 0.5757, Elapsed: 0m1s\n",
      "2022-02-25 18:31:14.676453: Epoch: 5, Batch: 42, Loss: 0.5341, Elapsed: 0m1s\n",
      "2022-02-25 18:31:16.272439: Epoch: 5, Batch: 43, Loss: 0.5418, Elapsed: 0m1s\n",
      "2022-02-25 18:31:17.761293: Epoch: 5, Batch: 44, Loss: 0.5523, Elapsed: 0m1s\n",
      "2022-02-25 18:31:19.276678: Epoch: 5, Batch: 45, Loss: 0.5672, Elapsed: 0m1s\n",
      "2022-02-25 18:31:20.852877: Epoch: 5, Batch: 46, Loss: 0.5142, Elapsed: 0m1s\n",
      "2022-02-25 18:31:22.000651: Epoch: 5, Batch: 47, Loss: 0.5965, Elapsed: 0m0s\n",
      "2022-02-25 18:31:23.704033: Epoch: 5, Batch: 48, Loss: 0.5241, Elapsed: 0m1s\n",
      "2022-02-25 18:31:25.996022: Epoch: 5, Batch: 49, Loss: 0.5002, Elapsed: 0m2s\n",
      "2022-02-25 18:31:27.296821: Epoch: 5, Batch: 50, Loss: 0.5587, Elapsed: 0m1s\n",
      "2022-02-25 18:31:27.515914 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:32:06.861897: validation Test:  Loss: 0.5125,  AUC: 0.6359, Acc: 77.4522,  Precision: 0.5778 -- Elapsed: 0m39s\n",
      "2022-02-25 18:32:06.861974 Starting testing the train set with 50 subgraphs!\n",
      "/home/jovyan/qtrkx-gnn-tracking/test.py:91: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  precision_7 = tp/(tp+fp) # also named purity\n",
      "2022-02-25 18:32:44.727620: training Test:  Loss: 0.5238,  AUC: 0.6302, Acc: 76.7695,  Precision: 0.6256 -- Elapsed: 0m37s\n",
      "2022-02-25 18:32:46.173595: Epoch: 6, Batch: 1, Loss: 0.5059, Elapsed: 0m1s\n",
      "2022-02-25 18:32:47.304358: Epoch: 6, Batch: 2, Loss: 0.5714, Elapsed: 0m0s\n",
      "2022-02-25 18:32:48.755279: Epoch: 6, Batch: 3, Loss: 0.5477, Elapsed: 0m1s\n",
      "2022-02-25 18:32:50.545618: Epoch: 6, Batch: 4, Loss: 0.4979, Elapsed: 0m1s\n",
      "2022-02-25 18:32:51.698071: Epoch: 6, Batch: 5, Loss: 0.5576, Elapsed: 0m0s\n",
      "2022-02-25 18:32:53.780886: Epoch: 6, Batch: 6, Loss: 0.5005, Elapsed: 0m1s\n",
      "2022-02-25 18:32:55.193885: Epoch: 6, Batch: 7, Loss: 0.5442, Elapsed: 0m1s\n",
      "2022-02-25 18:32:56.830590: Epoch: 6, Batch: 8, Loss: 0.5254, Elapsed: 0m1s\n",
      "2022-02-25 18:32:58.225344: Epoch: 6, Batch: 9, Loss: 0.5563, Elapsed: 0m1s\n",
      "2022-02-25 18:32:59.593176: Epoch: 6, Batch: 10, Loss: 0.5583, Elapsed: 0m1s\n",
      "2022-02-25 18:33:00.697589: Epoch: 6, Batch: 11, Loss: 0.6172, Elapsed: 0m0s\n",
      "2022-02-25 18:33:01.764393: Epoch: 6, Batch: 12, Loss: 0.5743, Elapsed: 0m0s\n",
      "2022-02-25 18:33:03.484407: Epoch: 6, Batch: 13, Loss: 0.5347, Elapsed: 0m1s\n",
      "2022-02-25 18:33:04.840657: Epoch: 6, Batch: 14, Loss: 0.5468, Elapsed: 0m1s\n",
      "2022-02-25 18:33:06.058931: Epoch: 6, Batch: 15, Loss: 0.5580, Elapsed: 0m1s\n",
      "2022-02-25 18:33:07.956136: Epoch: 6, Batch: 16, Loss: 0.5232, Elapsed: 0m1s\n",
      "2022-02-25 18:33:09.742256: Epoch: 6, Batch: 17, Loss: 0.5301, Elapsed: 0m1s\n",
      "2022-02-25 18:33:11.254236: Epoch: 6, Batch: 18, Loss: 0.5546, Elapsed: 0m1s\n",
      "2022-02-25 18:33:13.137763: Epoch: 6, Batch: 19, Loss: 0.5063, Elapsed: 0m1s\n",
      "2022-02-25 18:33:14.486748: Epoch: 6, Batch: 20, Loss: 0.5017, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:33:15.977104: Epoch: 6, Batch: 21, Loss: 0.5073, Elapsed: 0m1s\n",
      "2022-02-25 18:33:17.910417: Epoch: 6, Batch: 22, Loss: 0.5103, Elapsed: 0m1s\n",
      "2022-02-25 18:33:19.169994: Epoch: 6, Batch: 23, Loss: 0.5557, Elapsed: 0m1s\n",
      "2022-02-25 18:33:20.718202: Epoch: 6, Batch: 24, Loss: 0.5435, Elapsed: 0m1s\n",
      "2022-02-25 18:33:22.878935: Epoch: 6, Batch: 25, Loss: 0.4640, Elapsed: 0m1s\n",
      "2022-02-25 18:33:24.121243: Epoch: 6, Batch: 26, Loss: 0.5702, Elapsed: 0m1s\n",
      "2022-02-25 18:33:25.647706: Epoch: 6, Batch: 27, Loss: 0.5131, Elapsed: 0m1s\n",
      "2022-02-25 18:33:27.778629: Epoch: 6, Batch: 28, Loss: 0.4350, Elapsed: 0m1s\n",
      "2022-02-25 18:33:29.215771: Epoch: 6, Batch: 29, Loss: 0.5251, Elapsed: 0m1s\n",
      "2022-02-25 18:33:31.069211: Epoch: 6, Batch: 30, Loss: 0.4934, Elapsed: 0m1s\n",
      "2022-02-25 18:33:33.306874: Epoch: 6, Batch: 31, Loss: 0.4962, Elapsed: 0m2s\n",
      "2022-02-25 18:33:34.329625: Epoch: 6, Batch: 32, Loss: 0.5923, Elapsed: 0m0s\n",
      "2022-02-25 18:33:35.436427: Epoch: 6, Batch: 33, Loss: 0.5820, Elapsed: 0m0s\n",
      "2022-02-25 18:33:36.942354: Epoch: 6, Batch: 34, Loss: 0.5696, Elapsed: 0m1s\n",
      "2022-02-25 18:33:38.329142: Epoch: 6, Batch: 35, Loss: 0.5651, Elapsed: 0m1s\n",
      "2022-02-25 18:33:39.643197: Epoch: 6, Batch: 36, Loss: 0.5401, Elapsed: 0m1s\n",
      "2022-02-25 18:33:41.224413: Epoch: 6, Batch: 37, Loss: 0.5128, Elapsed: 0m1s\n",
      "2022-02-25 18:33:42.694253: Epoch: 6, Batch: 38, Loss: 0.5306, Elapsed: 0m1s\n",
      "2022-02-25 18:33:44.301792: Epoch: 6, Batch: 39, Loss: 0.5061, Elapsed: 0m1s\n",
      "2022-02-25 18:33:45.896763: Epoch: 6, Batch: 40, Loss: 0.5060, Elapsed: 0m1s\n",
      "2022-02-25 18:33:47.398733: Epoch: 6, Batch: 41, Loss: 0.5434, Elapsed: 0m1s\n",
      "2022-02-25 18:33:48.544068: Epoch: 6, Batch: 42, Loss: 0.5648, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:33:50.981452: Epoch: 6, Batch: 43, Loss: 0.4693, Elapsed: 0m2s\n",
      "2022-02-25 18:33:52.625358: Epoch: 6, Batch: 44, Loss: 0.5426, Elapsed: 0m1s\n",
      "2022-02-25 18:33:54.033538: Epoch: 6, Batch: 45, Loss: 0.5366, Elapsed: 0m1s\n",
      "2022-02-25 18:33:55.705897: Epoch: 6, Batch: 46, Loss: 0.5408, Elapsed: 0m1s\n",
      "2022-02-25 18:33:58.020258: Epoch: 6, Batch: 47, Loss: 0.4615, Elapsed: 0m2s\n",
      "2022-02-25 18:33:59.028871: Epoch: 6, Batch: 48, Loss: 0.5845, Elapsed: 0m0s\n",
      "2022-02-25 18:34:00.406029: Epoch: 6, Batch: 49, Loss: 0.5617, Elapsed: 0m1s\n",
      "2022-02-25 18:34:01.649825: Epoch: 6, Batch: 50, Loss: 0.5647, Elapsed: 0m1s\n",
      "2022-02-25 18:34:01.847323 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:34:41.204500: validation Test:  Loss: 0.5128,  AUC: 0.6449, Acc: 77.5633,  Precision: 0.5541 -- Elapsed: 0m39s\n",
      "2022-02-25 18:34:41.204588 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:35:18.964302: training Test:  Loss: 0.5235,  AUC: 0.6411, Acc: 76.7623,  Precision: 0.5610 -- Elapsed: 0m37s\n",
      "2022-02-25 18:35:20.382627: Epoch: 7, Batch: 1, Loss: 0.5269, Elapsed: 0m1s\n",
      "2022-02-25 18:35:21.786561: Epoch: 7, Batch: 2, Loss: 0.5233, Elapsed: 0m1s\n",
      "2022-02-25 18:35:23.384083: Epoch: 7, Batch: 3, Loss: 0.5372, Elapsed: 0m1s\n",
      "2022-02-25 18:35:25.014572: Epoch: 7, Batch: 4, Loss: 0.5096, Elapsed: 0m1s\n",
      "2022-02-25 18:35:26.057963: Epoch: 7, Batch: 5, Loss: 0.5691, Elapsed: 0m0s\n",
      "2022-02-25 18:35:28.341203: Epoch: 7, Batch: 6, Loss: 0.4659, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:35:30.496223: Epoch: 7, Batch: 7, Loss: 0.4379, Elapsed: 0m1s\n",
      "2022-02-25 18:35:31.885882: Epoch: 7, Batch: 8, Loss: 0.5441, Elapsed: 0m1s\n",
      "2022-02-25 18:35:33.404294: Epoch: 7, Batch: 9, Loss: 0.5101, Elapsed: 0m1s\n",
      "2022-02-25 18:35:34.575375: Epoch: 7, Batch: 10, Loss: 0.5840, Elapsed: 0m0s\n",
      "2022-02-25 18:35:35.966800: Epoch: 7, Batch: 11, Loss: 0.5546, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:35:37.455825: Epoch: 7, Batch: 12, Loss: 0.5337, Elapsed: 0m1s\n",
      "2022-02-25 18:35:39.051701: Epoch: 7, Batch: 13, Loss: 0.5076, Elapsed: 0m1s\n",
      "2022-02-25 18:35:40.180486: Epoch: 7, Batch: 14, Loss: 0.5768, Elapsed: 0m0s\n",
      "2022-02-25 18:35:42.427536: Epoch: 7, Batch: 15, Loss: 0.4969, Elapsed: 0m2s\n",
      "2022-02-25 18:35:43.869232: Epoch: 7, Batch: 16, Loss: 0.5608, Elapsed: 0m1s\n",
      "2022-02-25 18:35:44.929338: Epoch: 7, Batch: 17, Loss: 0.5936, Elapsed: 0m0s\n",
      "2022-02-25 18:35:47.051030: Epoch: 7, Batch: 18, Loss: 0.4624, Elapsed: 0m1s\n",
      "2022-02-25 18:35:48.551615: Epoch: 7, Batch: 19, Loss: 0.5332, Elapsed: 0m1s\n",
      "2022-02-25 18:35:49.747342: Epoch: 7, Batch: 20, Loss: 0.5563, Elapsed: 0m0s\n",
      "2022-02-25 18:35:51.295620: Epoch: 7, Batch: 21, Loss: 0.5379, Elapsed: 0m1s\n",
      "2022-02-25 18:35:52.572871: Epoch: 7, Batch: 22, Loss: 0.5553, Elapsed: 0m1s\n",
      "2022-02-25 18:35:53.822231: Epoch: 7, Batch: 23, Loss: 0.5472, Elapsed: 0m1s\n",
      "2022-02-25 18:35:55.786376: Epoch: 7, Batch: 24, Loss: 0.5054, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:35:57.026581: Epoch: 7, Batch: 25, Loss: 0.5547, Elapsed: 0m1s\n",
      "2022-02-25 18:35:58.901311: Epoch: 7, Batch: 26, Loss: 0.5157, Elapsed: 0m1s\n",
      "2022-02-25 18:36:00.291348: Epoch: 7, Batch: 27, Loss: 0.5543, Elapsed: 0m1s\n",
      "2022-02-25 18:36:01.608647: Epoch: 7, Batch: 28, Loss: 0.4979, Elapsed: 0m1s\n",
      "2022-02-25 18:36:02.605713: Epoch: 7, Batch: 29, Loss: 0.5841, Elapsed: 0m0s\n",
      "2022-02-25 18:36:05.003760: Epoch: 7, Batch: 30, Loss: 0.4704, Elapsed: 0m2s\n",
      "2022-02-25 18:36:06.119121: Epoch: 7, Batch: 31, Loss: 0.5697, Elapsed: 0m0s\n",
      "2022-02-25 18:36:07.602679: Epoch: 7, Batch: 32, Loss: 0.5433, Elapsed: 0m1s\n",
      "2022-02-25 18:36:09.043207: Epoch: 7, Batch: 33, Loss: 0.4983, Elapsed: 0m1s\n",
      "2022-02-25 18:36:10.780632: Epoch: 7, Batch: 34, Loss: 0.5411, Elapsed: 0m1s\n",
      "2022-02-25 18:36:12.741491: Epoch: 7, Batch: 35, Loss: 0.5024, Elapsed: 0m1s\n",
      "2022-02-25 18:36:14.170007: Epoch: 7, Batch: 36, Loss: 0.5638, Elapsed: 0m1s\n",
      "2022-02-25 18:36:15.609189: Epoch: 7, Batch: 37, Loss: 0.5456, Elapsed: 0m1s\n",
      "2022-02-25 18:36:17.302262: Epoch: 7, Batch: 38, Loss: 0.5274, Elapsed: 0m1s\n",
      "2022-02-25 18:36:19.274705: Epoch: 7, Batch: 39, Loss: 0.4972, Elapsed: 0m1s\n",
      "2022-02-25 18:36:21.028663: Epoch: 7, Batch: 40, Loss: 0.5197, Elapsed: 0m1s\n",
      "2022-02-25 18:36:22.347089: Epoch: 7, Batch: 41, Loss: 0.5607, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:36:24.209198: Epoch: 7, Batch: 42, Loss: 0.4884, Elapsed: 0m1s\n",
      "2022-02-25 18:36:25.885856: Epoch: 7, Batch: 43, Loss: 0.5365, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:36:27.241005: Epoch: 7, Batch: 44, Loss: 0.5558, Elapsed: 0m1s\n",
      "2022-02-25 18:36:28.860597: Epoch: 7, Batch: 45, Loss: 0.5071, Elapsed: 0m1s\n",
      "2022-02-25 18:36:29.882322: Epoch: 7, Batch: 46, Loss: 0.6181, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:36:31.196135: Epoch: 7, Batch: 47, Loss: 0.5401, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:36:32.483766: Epoch: 7, Batch: 48, Loss: 0.5777, Elapsed: 0m1s\n",
      "2022-02-25 18:36:34.211556: Epoch: 7, Batch: 49, Loss: 0.4943, Elapsed: 0m1s\n",
      "2022-02-25 18:36:35.784926: Epoch: 7, Batch: 50, Loss: 0.5094, Elapsed: 0m1s\n",
      "2022-02-25 18:36:35.985978 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:37:15.377313: validation Test:  Loss: 0.5085,  AUC: 0.6438, Acc: 77.7922,  Precision: 0.7121 -- Elapsed: 0m39s\n",
      "2022-02-25 18:37:15.377389 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:37:52.986103: training Test:  Loss: 0.5198,  AUC: 0.6383, Acc: 76.9657,  Precision: 0.7185 -- Elapsed: 0m37s\n",
      "2022-02-25 18:37:54.150559: Epoch: 8, Batch: 1, Loss: 0.5558, Elapsed: 0m1s\n",
      "2022-02-25 18:37:56.083700: Epoch: 8, Batch: 2, Loss: 0.5000, Elapsed: 0m1s\n",
      "2022-02-25 18:37:57.643517: Epoch: 8, Batch: 3, Loss: 0.5352, Elapsed: 0m1s\n",
      "2022-02-25 18:37:59.785161: Epoch: 8, Batch: 4, Loss: 0.4616, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:01.447224: Epoch: 8, Batch: 5, Loss: 0.5144, Elapsed: 0m1s\n",
      "2022-02-25 18:38:02.859955: Epoch: 8, Batch: 6, Loss: 0.5409, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:03.871779: Epoch: 8, Batch: 7, Loss: 0.6061, Elapsed: 0m0s\n",
      "2022-02-25 18:38:05.516056: Epoch: 8, Batch: 8, Loss: 0.5056, Elapsed: 0m1s\n",
      "2022-02-25 18:38:06.927649: Epoch: 8, Batch: 9, Loss: 0.5233, Elapsed: 0m1s\n",
      "2022-02-25 18:38:09.406678: Epoch: 8, Batch: 10, Loss: 0.4706, Elapsed: 0m2s\n",
      "2022-02-25 18:38:11.198046: Epoch: 8, Batch: 11, Loss: 0.5293, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:12.489059: Epoch: 8, Batch: 12, Loss: 0.5571, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:14.354709: Epoch: 8, Batch: 13, Loss: 0.4914, Elapsed: 0m1s\n",
      "2022-02-25 18:38:15.823591: Epoch: 8, Batch: 14, Loss: 0.5333, Elapsed: 0m1s\n",
      "2022-02-25 18:38:17.551100: Epoch: 8, Batch: 15, Loss: 0.4929, Elapsed: 0m1s\n",
      "2022-02-25 18:38:18.834069: Epoch: 8, Batch: 16, Loss: 0.5357, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:38:20.396656: Epoch: 8, Batch: 17, Loss: 0.5055, Elapsed: 0m1s\n",
      "2022-02-25 18:38:21.715476: Epoch: 8, Batch: 18, Loss: 0.4969, Elapsed: 0m1s\n",
      "2022-02-25 18:38:23.149758: Epoch: 8, Batch: 19, Loss: 0.5666, Elapsed: 0m1s\n",
      "2022-02-25 18:38:24.436559: Epoch: 8, Batch: 20, Loss: 0.5456, Elapsed: 0m1s\n",
      "2022-02-25 18:38:26.772569: Epoch: 8, Batch: 21, Loss: 0.4551, Elapsed: 0m2s\n",
      "2022-02-25 18:38:28.253599: Epoch: 8, Batch: 22, Loss: 0.5438, Elapsed: 0m1s\n",
      "2022-02-25 18:38:29.440024: Epoch: 8, Batch: 23, Loss: 0.5523, Elapsed: 0m0s\n",
      "2022-02-25 18:38:30.738920: Epoch: 8, Batch: 24, Loss: 0.5723, Elapsed: 0m1s\n",
      "2022-02-25 18:38:32.461051: Epoch: 8, Batch: 25, Loss: 0.5415, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:34.720790: Epoch: 8, Batch: 26, Loss: 0.4950, Elapsed: 0m2s\n",
      "2022-02-25 18:38:35.763391: Epoch: 8, Batch: 27, Loss: 0.5688, Elapsed: 0m0s\n",
      "2022-02-25 18:38:37.696129: Epoch: 8, Batch: 28, Loss: 0.5125, Elapsed: 0m1s\n",
      "2022-02-25 18:38:39.001574: Epoch: 8, Batch: 29, Loss: 0.5630, Elapsed: 0m1s\n",
      "2022-02-25 18:38:40.510722: Epoch: 8, Batch: 30, Loss: 0.5409, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:41.919768: Epoch: 8, Batch: 31, Loss: 0.5481, Elapsed: 0m1s\n",
      "2022-02-25 18:38:43.292829: Epoch: 8, Batch: 32, Loss: 0.5415, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:38:44.975189: Epoch: 8, Batch: 33, Loss: 0.5019, Elapsed: 0m1s\n",
      "2022-02-25 18:38:46.995985: Epoch: 8, Batch: 34, Loss: 0.4969, Elapsed: 0m1s\n",
      "2022-02-25 18:38:48.828379: Epoch: 8, Batch: 35, Loss: 0.5197, Elapsed: 0m1s\n",
      "2022-02-25 18:38:50.779599: Epoch: 8, Batch: 36, Loss: 0.5028, Elapsed: 0m1s\n",
      "2022-02-25 18:38:51.888299: Epoch: 8, Batch: 37, Loss: 0.5657, Elapsed: 0m0s\n",
      "2022-02-25 18:38:53.523191: Epoch: 8, Batch: 38, Loss: 0.5350, Elapsed: 0m1s\n",
      "2022-02-25 18:38:54.969527: Epoch: 8, Batch: 39, Loss: 0.5286, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:38:57.202212: Epoch: 8, Batch: 40, Loss: 0.4282, Elapsed: 0m2s\n",
      "2022-02-25 18:38:58.269472: Epoch: 8, Batch: 41, Loss: 0.5860, Elapsed: 0m0s\n",
      "2022-02-25 18:38:59.742071: Epoch: 8, Batch: 42, Loss: 0.4964, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:39:01.344740: Epoch: 8, Batch: 43, Loss: 0.5060, Elapsed: 0m1s\n",
      "2022-02-25 18:39:02.517528: Epoch: 8, Batch: 44, Loss: 0.5641, Elapsed: 0m0s\n",
      "2022-02-25 18:39:03.535960: Epoch: 8, Batch: 45, Loss: 0.5846, Elapsed: 0m0s\n",
      "2022-02-25 18:39:04.995520: Epoch: 8, Batch: 46, Loss: 0.5603, Elapsed: 0m1s\n",
      "2022-02-25 18:39:06.369113: Epoch: 8, Batch: 47, Loss: 0.5453, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:39:08.034615: Epoch: 8, Batch: 48, Loss: 0.5186, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:39:09.240368: Epoch: 8, Batch: 49, Loss: 0.5505, Elapsed: 0m1s\n",
      "2022-02-25 18:39:10.386181: Epoch: 8, Batch: 50, Loss: 0.5737, Elapsed: 0m0s\n",
      "2022-02-25 18:39:10.605028 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:39:50.210236: validation Test:  Loss: 0.5061,  AUC: 0.6489, Acc: 77.9887,  Precision: 0.6254 -- Elapsed: 0m39s\n",
      "2022-02-25 18:39:50.210313 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:40:27.912416: training Test:  Loss: 0.5173,  AUC: 0.6430, Acc: 77.1386,  Precision: 0.6320 -- Elapsed: 0m37s\n",
      "2022-02-25 18:40:29.463337: Epoch: 9, Batch: 1, Loss: 0.4908, Elapsed: 0m1s\n",
      "2022-02-25 18:40:30.930972: Epoch: 9, Batch: 2, Loss: 0.5644, Elapsed: 0m1s\n",
      "2022-02-25 18:40:32.523450: Epoch: 9, Batch: 3, Loss: 0.5078, Elapsed: 0m1s\n",
      "2022-02-25 18:40:33.956255: Epoch: 9, Batch: 4, Loss: 0.5306, Elapsed: 0m1s\n",
      "2022-02-25 18:40:35.128598: Epoch: 9, Batch: 5, Loss: 0.5728, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:40:36.538545: Epoch: 9, Batch: 6, Loss: 0.5404, Elapsed: 0m1s\n",
      "2022-02-25 18:40:39.007285: Epoch: 9, Batch: 7, Loss: 0.4674, Elapsed: 0m2s\n",
      "2022-02-25 18:40:40.009505: Epoch: 9, Batch: 8, Loss: 0.5748, Elapsed: 0m0s\n",
      "2022-02-25 18:40:41.484470: Epoch: 9, Batch: 9, Loss: 0.5431, Elapsed: 0m1s\n",
      "2022-02-25 18:40:42.873521: Epoch: 9, Batch: 10, Loss: 0.5447, Elapsed: 0m1s\n",
      "2022-02-25 18:40:44.580519: Epoch: 9, Batch: 11, Loss: 0.5375, Elapsed: 0m1s\n",
      "2022-02-25 18:40:46.482908: Epoch: 9, Batch: 12, Loss: 0.5107, Elapsed: 0m1s\n",
      "2022-02-25 18:40:47.749023: Epoch: 9, Batch: 13, Loss: 0.5500, Elapsed: 0m1s\n",
      "2022-02-25 18:40:49.189706: Epoch: 9, Batch: 14, Loss: 0.5201, Elapsed: 0m1s\n",
      "2022-02-25 18:40:50.881007: Epoch: 9, Batch: 15, Loss: 0.5282, Elapsed: 0m1s\n",
      "2022-02-25 18:40:52.339039: Epoch: 9, Batch: 16, Loss: 0.5250, Elapsed: 0m1s\n",
      "2022-02-25 18:40:53.988327: Epoch: 9, Batch: 17, Loss: 0.5164, Elapsed: 0m1s\n",
      "2022-02-25 18:40:56.163380: Epoch: 9, Batch: 18, Loss: 0.4325, Elapsed: 0m1s\n",
      "2022-02-25 18:40:57.687597: Epoch: 9, Batch: 19, Loss: 0.4958, Elapsed: 0m1s\n",
      "2022-02-25 18:40:58.869590: Epoch: 9, Batch: 20, Loss: 0.5500, Elapsed: 0m0s\n",
      "2022-02-25 18:41:00.812101: Epoch: 9, Batch: 21, Loss: 0.5038, Elapsed: 0m1s\n",
      "2022-02-25 18:41:02.866024: Epoch: 9, Batch: 22, Loss: 0.4957, Elapsed: 0m1s\n",
      "2022-02-25 18:41:05.136094: Epoch: 9, Batch: 23, Loss: 0.4546, Elapsed: 0m2s\n",
      "2022-02-25 18:41:06.780412: Epoch: 9, Batch: 24, Loss: 0.5186, Elapsed: 0m1s\n",
      "2022-02-25 18:41:08.921067: Epoch: 9, Batch: 25, Loss: 0.4648, Elapsed: 0m1s\n",
      "2022-02-25 18:41:10.423831: Epoch: 9, Batch: 26, Loss: 0.5363, Elapsed: 0m1s\n",
      "2022-02-25 18:41:11.882705: Epoch: 9, Batch: 27, Loss: 0.5523, Elapsed: 0m1s\n",
      "2022-02-25 18:41:13.522340: Epoch: 9, Batch: 28, Loss: 0.5026, Elapsed: 0m1s\n",
      "2022-02-25 18:41:14.879151: Epoch: 9, Batch: 29, Loss: 0.5374, Elapsed: 0m1s\n",
      "2022-02-25 18:41:16.794648: Epoch: 9, Batch: 30, Loss: 0.4943, Elapsed: 0m1s\n",
      "2022-02-25 18:41:18.132977: Epoch: 9, Batch: 31, Loss: 0.5616, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 18:41:19.277940: Epoch: 9, Batch: 32, Loss: 0.5625, Elapsed: 0m0s\n",
      "2022-02-25 18:41:21.180014: Epoch: 9, Batch: 33, Loss: 0.5279, Elapsed: 0m1s\n",
      "2022-02-25 18:41:23.426397: Epoch: 9, Batch: 34, Loss: 0.4947, Elapsed: 0m2s\n",
      "2022-02-25 18:41:24.992203: Epoch: 9, Batch: 35, Loss: 0.5083, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:41:26.894231: Epoch: 9, Batch: 36, Loss: 0.4978, Elapsed: 0m1s\n",
      "2022-02-25 18:41:28.256641: Epoch: 9, Batch: 37, Loss: 0.5485, Elapsed: 0m1s\n",
      "2022-02-25 18:41:29.550940: Epoch: 9, Batch: 38, Loss: 0.4971, Elapsed: 0m1s\n",
      "2022-02-25 18:41:30.519852: Epoch: 9, Batch: 39, Loss: 0.6175, Elapsed: 0m0s\n",
      "2022-02-25 18:41:32.110855: Epoch: 9, Batch: 40, Loss: 0.5019, Elapsed: 0m1s\n",
      "2022-02-25 18:41:33.394340: Epoch: 9, Batch: 41, Loss: 0.5540, Elapsed: 0m1s\n",
      "2022-02-25 18:41:34.447347: Epoch: 9, Batch: 42, Loss: 0.5595, Elapsed: 0m0s\n",
      "2022-02-25 18:41:35.539068: Epoch: 9, Batch: 43, Loss: 0.5633, Elapsed: 0m0s\n",
      "2022-02-25 18:41:36.910224: Epoch: 9, Batch: 44, Loss: 0.5589, Elapsed: 0m1s\n",
      "2022-02-25 18:41:38.405533: Epoch: 9, Batch: 45, Loss: 0.5554, Elapsed: 0m1s\n",
      "2022-02-25 18:41:39.474860: Epoch: 9, Batch: 46, Loss: 0.5829, Elapsed: 0m0s\n",
      "2022-02-25 18:41:41.060713: Epoch: 9, Batch: 47, Loss: 0.5333, Elapsed: 0m1s\n",
      "2022-02-25 18:41:42.719213: Epoch: 9, Batch: 48, Loss: 0.5358, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:41:43.973667: Epoch: 9, Batch: 49, Loss: 0.5483, Elapsed: 0m1s\n",
      "2022-02-25 18:41:45.243888: Epoch: 9, Batch: 50, Loss: 0.5651, Elapsed: 0m1s\n",
      "2022-02-25 18:41:45.455166 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:42:24.780847: validation Test:  Loss: 0.5077,  AUC: 0.6486, Acc: 78.0280,  Precision: 0.6232 -- Elapsed: 0m39s\n",
      "2022-02-25 18:42:24.780923 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:43:02.584270: training Test:  Loss: 0.5182,  AUC: 0.6456, Acc: 77.0828,  Precision: 0.6158 -- Elapsed: 0m37s\n",
      "2022-02-25 18:43:03.996908: Epoch: 10, Batch: 1, Loss: 0.5188, Elapsed: 0m1s\n",
      "2022-02-25 18:43:05.421292: Epoch: 10, Batch: 2, Loss: 0.5253, Elapsed: 0m1s\n",
      "2022-02-25 18:43:06.912435: Epoch: 10, Batch: 3, Loss: 0.4954, Elapsed: 0m1s\n",
      "2022-02-25 18:43:08.454075: Epoch: 10, Batch: 4, Loss: 0.5361, Elapsed: 0m1s\n",
      "2022-02-25 18:43:09.711547: Epoch: 10, Batch: 5, Loss: 0.5701, Elapsed: 0m1s\n",
      "2022-02-25 18:43:11.131413: Epoch: 10, Batch: 6, Loss: 0.5530, Elapsed: 0m1s\n",
      "2022-02-25 18:43:12.903276: Epoch: 10, Batch: 7, Loss: 0.5280, Elapsed: 0m1s\n",
      "2022-02-25 18:43:15.205526: Epoch: 10, Batch: 8, Loss: 0.4948, Elapsed: 0m2s\n",
      "2022-02-25 18:43:16.632953: Epoch: 10, Batch: 9, Loss: 0.5421, Elapsed: 0m1s\n",
      "2022-02-25 18:43:18.213487: Epoch: 10, Batch: 10, Loss: 0.5322, Elapsed: 0m1s\n",
      "2022-02-25 18:43:20.455509: Epoch: 10, Batch: 11, Loss: 0.4286, Elapsed: 0m2s\n",
      "2022-02-25 18:43:21.922021: Epoch: 10, Batch: 12, Loss: 0.5587, Elapsed: 0m1s\n",
      "2022-02-25 18:43:23.816447: Epoch: 10, Batch: 13, Loss: 0.4870, Elapsed: 0m1s\n",
      "2022-02-25 18:43:25.181278: Epoch: 10, Batch: 14, Loss: 0.5554, Elapsed: 0m1s\n",
      "2022-02-25 18:43:26.523019: Epoch: 10, Batch: 15, Loss: 0.5326, Elapsed: 0m1s\n",
      "2022-02-25 18:43:28.361611: Epoch: 10, Batch: 16, Loss: 0.5184, Elapsed: 0m1s\n",
      "2022-02-25 18:43:30.027084: Epoch: 10, Batch: 17, Loss: 0.5021, Elapsed: 0m1s\n",
      "2022-02-25 18:43:31.048037: Epoch: 10, Batch: 18, Loss: 0.6084, Elapsed: 0m0s\n",
      "2022-02-25 18:43:32.980693: Epoch: 10, Batch: 19, Loss: 0.5036, Elapsed: 0m1s\n",
      "2022-02-25 18:43:34.660687: Epoch: 10, Batch: 20, Loss: 0.5335, Elapsed: 0m1s\n",
      "2022-02-25 18:43:36.740796: Epoch: 10, Batch: 21, Loss: 0.4957, Elapsed: 0m1s\n",
      "2022-02-25 18:43:38.044484: Epoch: 10, Batch: 22, Loss: 0.5575, Elapsed: 0m1s\n",
      "2022-02-25 18:43:40.414742: Epoch: 10, Batch: 23, Loss: 0.4497, Elapsed: 0m2s\n",
      "2022-02-25 18:43:41.441649: Epoch: 10, Batch: 24, Loss: 0.5760, Elapsed: 0m0s\n",
      "2022-02-25 18:43:42.674314: Epoch: 10, Batch: 25, Loss: 0.5551, Elapsed: 0m1s\n",
      "2022-02-25 18:43:44.017677: Epoch: 10, Batch: 26, Loss: 0.4994, Elapsed: 0m1s\n",
      "2022-02-25 18:43:45.291506: Epoch: 10, Batch: 27, Loss: 0.5446, Elapsed: 0m1s\n",
      "2022-02-25 18:43:46.946358: Epoch: 10, Batch: 28, Loss: 0.4978, Elapsed: 0m1s\n",
      "2022-02-25 18:43:48.235807: Epoch: 10, Batch: 29, Loss: 0.5671, Elapsed: 0m1s\n",
      "2022-02-25 18:43:50.031229: Epoch: 10, Batch: 30, Loss: 0.4882, Elapsed: 0m1s\n",
      "2022-02-25 18:43:51.707200: Epoch: 10, Batch: 31, Loss: 0.5051, Elapsed: 0m1s\n",
      "2022-02-25 18:43:53.175904: Epoch: 10, Batch: 32, Loss: 0.5640, Elapsed: 0m1s\n",
      "2022-02-25 18:43:54.297578: Epoch: 10, Batch: 33, Loss: 0.5662, Elapsed: 0m0s\n",
      "2022-02-25 18:43:55.712374: Epoch: 10, Batch: 34, Loss: 0.5453, Elapsed: 0m1s\n",
      "2022-02-25 18:43:57.157323: Epoch: 10, Batch: 35, Loss: 0.5422, Elapsed: 0m1s\n",
      "2022-02-25 18:43:58.243623: Epoch: 10, Batch: 36, Loss: 0.5619, Elapsed: 0m0s\n",
      "2022-02-25 18:43:59.732774: Epoch: 10, Batch: 37, Loss: 0.5266, Elapsed: 0m1s\n",
      "2022-02-25 18:44:01.195769: Epoch: 10, Batch: 38, Loss: 0.5430, Elapsed: 0m1s\n",
      "2022-02-25 18:44:02.335046: Epoch: 10, Batch: 39, Loss: 0.5758, Elapsed: 0m0s\n",
      "2022-02-25 18:44:04.444426: Epoch: 10, Batch: 40, Loss: 0.4593, Elapsed: 0m1s\n",
      "2022-02-25 18:44:06.071087: Epoch: 10, Batch: 41, Loss: 0.5014, Elapsed: 0m1s\n",
      "2022-02-25 18:44:07.260403: Epoch: 10, Batch: 42, Loss: 0.5478, Elapsed: 0m0s\n",
      "2022-02-25 18:44:09.358382: Epoch: 10, Batch: 43, Loss: 0.4976, Elapsed: 0m1s\n",
      "2022-02-25 18:44:11.834371: Epoch: 10, Batch: 44, Loss: 0.4632, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 2 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:44:13.753560: Epoch: 10, Batch: 45, Loss: 0.5082, Elapsed: 0m1s\n",
      "2022-02-25 18:44:15.108067: Epoch: 10, Batch: 46, Loss: 0.5465, Elapsed: 0m1s\n",
      "2022-02-25 18:44:16.774013: Epoch: 10, Batch: 47, Loss: 0.5352, Elapsed: 0m1s\n",
      "2022-02-25 18:44:18.220326: Epoch: 10, Batch: 48, Loss: 0.5295, Elapsed: 0m1s\n",
      "2022-02-25 18:44:19.780409: Epoch: 10, Batch: 49, Loss: 0.5061, Elapsed: 0m1s\n",
      "2022-02-25 18:44:20.799041: Epoch: 10, Batch: 50, Loss: 0.5799, Elapsed: 0m0s\n",
      "2022-02-25 18:44:21.009887 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:45:01.019644: validation Test:  Loss: 0.5053,  AUC: 0.6481, Acc: 78.0604,  Precision: 0.6912 -- Elapsed: 0m40s\n",
      "2022-02-25 18:45:01.019719 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:45:39.036790: training Test:  Loss: 0.5165,  AUC: 0.6446, Acc: 77.2034,  Precision: 0.7088 -- Elapsed: 0m38s\n",
      "2022-02-25 18:45:39.834181: Epoch: 11, Batch: 1, Loss: 0.5707, Elapsed: 0m0s\n",
      "2022-02-25 18:45:41.426677: Epoch: 11, Batch: 2, Loss: 0.4948, Elapsed: 0m1s\n",
      "2022-02-25 18:45:43.051706: Epoch: 11, Batch: 3, Loss: 0.5393, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:45:44.152747: Epoch: 11, Batch: 4, Loss: 0.5588, Elapsed: 0m0s\n",
      "2022-02-25 18:45:45.859769: Epoch: 11, Batch: 5, Loss: 0.4959, Elapsed: 0m1s\n",
      "2022-02-25 18:45:47.214794: Epoch: 11, Batch: 6, Loss: 0.5320, Elapsed: 0m1s\n",
      "2022-02-25 18:45:48.875183: Epoch: 11, Batch: 7, Loss: 0.5332, Elapsed: 0m1s\n",
      "2022-02-25 18:45:51.334583: Epoch: 11, Batch: 8, Loss: 0.4652, Elapsed: 0m2s\n",
      "2022-02-25 18:45:52.782462: Epoch: 11, Batch: 9, Loss: 0.5410, Elapsed: 0m1s\n",
      "2022-02-25 18:45:54.143654: Epoch: 11, Batch: 10, Loss: 0.4947, Elapsed: 0m1s\n",
      "2022-02-25 18:45:56.466303: Epoch: 11, Batch: 11, Loss: 0.4491, Elapsed: 0m2s\n",
      "2022-02-25 18:45:58.654137: Epoch: 11, Batch: 12, Loss: 0.4583, Elapsed: 0m1s\n",
      "2022-02-25 18:46:00.566319: Epoch: 11, Batch: 13, Loss: 0.4844, Elapsed: 0m1s\n",
      "2022-02-25 18:46:01.957135: Epoch: 11, Batch: 14, Loss: 0.5474, Elapsed: 0m1s\n",
      "2022-02-25 18:46:03.303789: Epoch: 11, Batch: 15, Loss: 0.5508, Elapsed: 0m1s\n",
      "2022-02-25 18:46:05.307277: Epoch: 11, Batch: 16, Loss: 0.4944, Elapsed: 0m1s\n",
      "2022-02-25 18:46:06.869592: Epoch: 11, Batch: 17, Loss: 0.5322, Elapsed: 0m1s\n",
      "2022-02-25 18:46:08.823767: Epoch: 11, Batch: 18, Loss: 0.5081, Elapsed: 0m1s\n",
      "2022-02-25 18:46:10.429959: Epoch: 11, Batch: 19, Loss: 0.5161, Elapsed: 0m1s\n",
      "2022-02-25 18:46:12.174001: Epoch: 11, Batch: 20, Loss: 0.4870, Elapsed: 0m1s\n",
      "2022-02-25 18:46:13.612450: Epoch: 11, Batch: 21, Loss: 0.5591, Elapsed: 0m1s\n",
      "2022-02-25 18:46:14.795048: Epoch: 11, Batch: 22, Loss: 0.5619, Elapsed: 0m0s\n",
      "2022-02-25 18:46:16.396964: Epoch: 11, Batch: 23, Loss: 0.5052, Elapsed: 0m1s\n",
      "2022-02-25 18:46:17.478056: Epoch: 11, Batch: 24, Loss: 0.5791, Elapsed: 0m0s\n",
      "2022-02-25 18:46:19.379962: Epoch: 11, Batch: 25, Loss: 0.4981, Elapsed: 0m1s\n",
      "2022-02-25 18:46:20.718217: Epoch: 11, Batch: 26, Loss: 0.5559, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:46:22.242808: Epoch: 11, Batch: 27, Loss: 0.5034, Elapsed: 0m1s\n",
      "2022-02-25 18:46:23.964173: Epoch: 11, Batch: 28, Loss: 0.5234, Elapsed: 0m1s\n",
      "2022-02-25 18:46:25.080090: Epoch: 11, Batch: 29, Loss: 0.5656, Elapsed: 0m0s\n",
      "2022-02-25 18:46:26.272315: Epoch: 11, Batch: 30, Loss: 0.5493, Elapsed: 0m0s\n",
      "2022-02-25 18:46:27.757231: Epoch: 11, Batch: 31, Loss: 0.5188, Elapsed: 0m1s\n",
      "2022-02-25 18:46:29.320429: Epoch: 11, Batch: 32, Loss: 0.5032, Elapsed: 0m1s\n",
      "2022-02-25 18:46:30.580592: Epoch: 11, Batch: 33, Loss: 0.5566, Elapsed: 0m1s\n",
      "2022-02-25 18:46:32.856488: Epoch: 11, Batch: 34, Loss: 0.4284, Elapsed: 0m2s\n",
      "2022-02-25 18:46:34.089417: Epoch: 11, Batch: 35, Loss: 0.5445, Elapsed: 0m1s\n",
      "2022-02-25 18:46:36.010990: Epoch: 11, Batch: 36, Loss: 0.5205, Elapsed: 0m1s\n",
      "2022-02-25 18:46:37.523899: Epoch: 11, Batch: 37, Loss: 0.5243, Elapsed: 0m1s\n",
      "2022-02-25 18:46:39.270708: Epoch: 11, Batch: 38, Loss: 0.5345, Elapsed: 0m1s\n",
      "2022-02-25 18:46:40.439117: Epoch: 11, Batch: 39, Loss: 0.5689, Elapsed: 0m0s\n",
      "2022-02-25 18:46:41.704600: Epoch: 11, Batch: 40, Loss: 0.5458, Elapsed: 0m1s\n",
      "2022-02-25 18:46:43.046990: Epoch: 11, Batch: 41, Loss: 0.5422, Elapsed: 0m1s\n",
      "2022-02-25 18:46:44.498455: Epoch: 11, Batch: 42, Loss: 0.5420, Elapsed: 0m1s\n",
      "2022-02-25 18:46:45.813722: Epoch: 11, Batch: 43, Loss: 0.5603, Elapsed: 0m1s\n",
      "2022-02-25 18:46:47.466955: Epoch: 11, Batch: 44, Loss: 0.5008, Elapsed: 0m1s\n",
      "2022-02-25 18:46:48.917660: Epoch: 11, Batch: 45, Loss: 0.5404, Elapsed: 0m1s\n",
      "2022-02-25 18:46:51.219405: Epoch: 11, Batch: 46, Loss: 0.4945, Elapsed: 0m2s\n",
      "2022-02-25 18:46:52.721140: Epoch: 11, Batch: 47, Loss: 0.5284, Elapsed: 0m1s\n",
      "2022-02-25 18:46:54.705736: Epoch: 11, Batch: 48, Loss: 0.5035, Elapsed: 0m1s\n",
      "2022-02-25 18:46:56.174652: Epoch: 11, Batch: 49, Loss: 0.5647, Elapsed: 0m1s\n",
      "2022-02-25 18:46:57.209508: Epoch: 11, Batch: 50, Loss: 0.6018, Elapsed: 0m0s\n",
      "2022-02-25 18:46:57.432625 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:47:37.580576: validation Test:  Loss: 0.5037,  AUC: 0.6529, Acc: 78.1971,  Precision: 0.6209 -- Elapsed: 0m40s\n",
      "2022-02-25 18:47:37.580650 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:48:15.735429: training Test:  Loss: 0.5143,  AUC: 0.6493, Acc: 77.3276,  Precision: 0.6278 -- Elapsed: 0m38s\n",
      "2022-02-25 18:48:16.767406: Epoch: 12, Batch: 1, Loss: 0.5487, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:48:19.101979: Epoch: 12, Batch: 2, Loss: 0.4932, Elapsed: 0m2s\n",
      "2022-02-25 18:48:20.691202: Epoch: 12, Batch: 3, Loss: 0.5052, Elapsed: 0m1s\n",
      "2022-02-25 18:48:22.341974: Epoch: 12, Batch: 4, Loss: 0.5007, Elapsed: 0m1s\n",
      "2022-02-25 18:48:24.166958: Epoch: 12, Batch: 5, Loss: 0.5223, Elapsed: 0m1s\n",
      "2022-02-25 18:48:25.548083: Epoch: 12, Batch: 6, Loss: 0.5473, Elapsed: 0m1s\n",
      "2022-02-25 18:48:27.452230: Epoch: 12, Batch: 7, Loss: 0.5048, Elapsed: 0m1s\n",
      "2022-02-25 18:48:28.585160: Epoch: 12, Batch: 8, Loss: 0.5665, Elapsed: 0m0s\n",
      "2022-02-25 18:48:30.422789: Epoch: 12, Batch: 9, Loss: 0.4877, Elapsed: 0m1s\n",
      "2022-02-25 18:48:31.483111: Epoch: 12, Batch: 10, Loss: 0.5964, Elapsed: 0m0s\n",
      "2022-02-25 18:48:33.640838: Epoch: 12, Batch: 11, Loss: 0.4944, Elapsed: 0m1s\n",
      "2022-02-25 18:48:34.926759: Epoch: 12, Batch: 12, Loss: 0.5597, Elapsed: 0m1s\n",
      "2022-02-25 18:48:37.197492: Epoch: 12, Batch: 13, Loss: 0.4470, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:48:39.135420: Epoch: 12, Batch: 14, Loss: 0.4865, Elapsed: 0m1s\n",
      "2022-02-25 18:48:40.388835: Epoch: 12, Batch: 15, Loss: 0.5473, Elapsed: 0m1s\n",
      "2022-02-25 18:48:41.945423: Epoch: 12, Batch: 16, Loss: 0.5228, Elapsed: 0m1s\n",
      "2022-02-25 18:48:43.361418: Epoch: 12, Batch: 17, Loss: 0.5177, Elapsed: 0m1s\n",
      "2022-02-25 18:48:44.791015: Epoch: 12, Batch: 18, Loss: 0.5443, Elapsed: 0m1s\n",
      "2022-02-25 18:48:46.513927: Epoch: 12, Batch: 19, Loss: 0.5255, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:48:47.924640: Epoch: 12, Batch: 20, Loss: 0.5434, Elapsed: 0m1s\n",
      "2022-02-25 18:48:49.413892: Epoch: 12, Batch: 21, Loss: 0.5430, Elapsed: 0m1s\n",
      "2022-02-25 18:48:50.609946: Epoch: 12, Batch: 22, Loss: 0.5433, Elapsed: 0m0s\n",
      "2022-02-25 18:48:52.051303: Epoch: 12, Batch: 23, Loss: 0.4935, Elapsed: 0m1s\n",
      "2022-02-25 18:48:53.972298: Epoch: 12, Batch: 24, Loss: 0.4964, Elapsed: 0m1s\n",
      "2022-02-25 18:48:55.333642: Epoch: 12, Batch: 25, Loss: 0.5297, Elapsed: 0m1s\n",
      "2022-02-25 18:48:56.400510: Epoch: 12, Batch: 26, Loss: 0.5779, Elapsed: 0m0s\n",
      "2022-02-25 18:48:58.621856: Epoch: 12, Batch: 27, Loss: 0.4245, Elapsed: 0m1s\n",
      "2022-02-25 18:49:00.057478: Epoch: 12, Batch: 28, Loss: 0.5502, Elapsed: 0m1s\n",
      "2022-02-25 18:49:01.067471: Epoch: 12, Batch: 29, Loss: 0.5632, Elapsed: 0m0s\n",
      "2022-02-25 18:49:02.738136: Epoch: 12, Batch: 30, Loss: 0.5345, Elapsed: 0m1s\n",
      "2022-02-25 18:49:03.831637: Epoch: 12, Batch: 31, Loss: 0.5596, Elapsed: 0m0s\n",
      "2022-02-25 18:49:05.414943: Epoch: 12, Batch: 32, Loss: 0.5315, Elapsed: 0m1s\n",
      "2022-02-25 18:49:07.085681: Epoch: 12, Batch: 33, Loss: 0.5142, Elapsed: 0m1s\n",
      "2022-02-25 18:49:08.848057: Epoch: 12, Batch: 34, Loss: 0.5349, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:49:11.061377: Epoch: 12, Batch: 35, Loss: 0.4573, Elapsed: 0m1s\n",
      "2022-02-25 18:49:12.491199: Epoch: 12, Batch: 36, Loss: 0.5523, Elapsed: 0m1s\n",
      "2022-02-25 18:49:13.949116: Epoch: 12, Batch: 37, Loss: 0.5534, Elapsed: 0m1s\n",
      "2022-02-25 18:49:15.630160: Epoch: 12, Batch: 38, Loss: 0.4950, Elapsed: 0m1s\n",
      "2022-02-25 18:49:17.594151: Epoch: 12, Batch: 39, Loss: 0.5092, Elapsed: 0m1s\n",
      "2022-02-25 18:49:18.882613: Epoch: 12, Batch: 40, Loss: 0.5539, Elapsed: 0m1s\n",
      "2022-02-25 18:49:20.036977: Epoch: 12, Batch: 41, Loss: 0.5612, Elapsed: 0m0s\n",
      "2022-02-25 18:49:21.566261: Epoch: 12, Batch: 42, Loss: 0.5637, Elapsed: 0m1s\n",
      "2022-02-25 18:49:23.214181: Epoch: 12, Batch: 43, Loss: 0.5042, Elapsed: 0m1s\n",
      "2022-02-25 18:49:24.680256: Epoch: 12, Batch: 44, Loss: 0.5394, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:49:27.196151: Epoch: 12, Batch: 45, Loss: 0.4641, Elapsed: 0m2s\n",
      "2022-02-25 18:49:28.737028: Epoch: 12, Batch: 46, Loss: 0.5275, Elapsed: 0m1s\n",
      "2022-02-25 18:49:30.156580: Epoch: 12, Batch: 47, Loss: 0.5427, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:49:31.374115: Epoch: 12, Batch: 48, Loss: 0.5685, Elapsed: 0m1s\n",
      "2022-02-25 18:49:33.004999: Epoch: 12, Batch: 49, Loss: 0.5005, Elapsed: 0m1s\n",
      "2022-02-25 18:49:34.369175: Epoch: 12, Batch: 50, Loss: 0.4873, Elapsed: 0m1s\n",
      "2022-02-25 18:49:34.581881 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:50:14.915495: validation Test:  Loss: 0.5039,  AUC: 0.6490, Acc: 78.1954,  Precision: 0.6640 -- Elapsed: 0m40s\n",
      "2022-02-25 18:50:14.915580 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:50:53.635868: training Test:  Loss: 0.5152,  AUC: 0.6457, Acc: 77.3979,  Precision: 0.6910 -- Elapsed: 0m38s\n",
      "2022-02-25 18:50:54.842914: Epoch: 13, Batch: 1, Loss: 0.5517, Elapsed: 0m1s\n",
      "2022-02-25 18:50:56.733870: Epoch: 13, Batch: 2, Loss: 0.4849, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:50:58.196543: Epoch: 13, Batch: 3, Loss: 0.5399, Elapsed: 0m1s\n",
      "2022-02-25 18:50:59.780288: Epoch: 13, Batch: 4, Loss: 0.5314, Elapsed: 0m1s\n",
      "2022-02-25 18:51:01.501081: Epoch: 13, Batch: 5, Loss: 0.4945, Elapsed: 0m1s\n",
      "2022-02-25 18:51:03.525527: Epoch: 13, Batch: 6, Loss: 0.4935, Elapsed: 0m1s\n",
      "2022-02-25 18:51:04.777121: Epoch: 13, Batch: 7, Loss: 0.5503, Elapsed: 0m1s\n",
      "2022-02-25 18:51:05.970641: Epoch: 13, Batch: 8, Loss: 0.5656, Elapsed: 0m0s\n",
      "2022-02-25 18:51:07.444038: Epoch: 13, Batch: 9, Loss: 0.5392, Elapsed: 0m1s\n",
      "2022-02-25 18:51:08.770239: Epoch: 13, Batch: 10, Loss: 0.5322, Elapsed: 0m1s\n",
      "2022-02-25 18:51:10.422327: Epoch: 13, Batch: 11, Loss: 0.5010, Elapsed: 0m1s\n",
      "2022-02-25 18:51:11.935300: Epoch: 13, Batch: 12, Loss: 0.5398, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:51:13.402542: Epoch: 13, Batch: 13, Loss: 0.5291, Elapsed: 0m1s\n",
      "2022-02-25 18:51:14.425846: Epoch: 13, Batch: 14, Loss: 0.5979, Elapsed: 0m0s\n",
      "2022-02-25 18:51:16.351975: Epoch: 13, Batch: 15, Loss: 0.5086, Elapsed: 0m1s\n",
      "2022-02-25 18:51:17.860604: Epoch: 13, Batch: 16, Loss: 0.5635, Elapsed: 0m1s\n",
      "2022-02-25 18:51:19.463263: Epoch: 13, Batch: 17, Loss: 0.5064, Elapsed: 0m1s\n",
      "2022-02-25 18:51:20.462873: Epoch: 13, Batch: 18, Loss: 0.5620, Elapsed: 0m0s\n",
      "2022-02-25 18:51:22.163335: Epoch: 13, Batch: 19, Loss: 0.5229, Elapsed: 0m1s\n",
      "2022-02-25 18:51:23.586544: Epoch: 13, Batch: 20, Loss: 0.4968, Elapsed: 0m1s\n",
      "2022-02-25 18:51:25.241030: Epoch: 13, Batch: 21, Loss: 0.5303, Elapsed: 0m1s\n",
      "2022-02-25 18:51:26.574013: Epoch: 13, Batch: 22, Loss: 0.4913, Elapsed: 0m1s\n",
      "2022-02-25 18:51:27.778469: Epoch: 13, Batch: 23, Loss: 0.5729, Elapsed: 0m0s\n",
      "2022-02-25 18:51:29.171108: Epoch: 13, Batch: 24, Loss: 0.5431, Elapsed: 0m1s\n",
      "2022-02-25 18:51:30.618455: Epoch: 13, Batch: 25, Loss: 0.5495, Elapsed: 0m1s\n",
      "2022-02-25 18:51:31.709024: Epoch: 13, Batch: 26, Loss: 0.5809, Elapsed: 0m0s\n",
      "2022-02-25 18:51:32.835247: Epoch: 13, Batch: 27, Loss: 0.5660, Elapsed: 0m0s\n",
      "2022-02-25 18:51:35.012812: Epoch: 13, Batch: 28, Loss: 0.4268, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:51:37.323036: Epoch: 13, Batch: 29, Loss: 0.4520, Elapsed: 0m2s\n",
      "2022-02-25 18:51:38.892403: Epoch: 13, Batch: 30, Loss: 0.5541, Elapsed: 0m1s\n",
      "2022-02-25 18:51:40.743518: Epoch: 13, Batch: 31, Loss: 0.5217, Elapsed: 0m1s\n",
      "2022-02-25 18:51:42.500818: Epoch: 13, Batch: 32, Loss: 0.4872, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:51:43.616971: Epoch: 13, Batch: 33, Loss: 0.5558, Elapsed: 0m0s\n",
      "2022-02-25 18:51:45.628099: Epoch: 13, Batch: 34, Loss: 0.4976, Elapsed: 0m1s\n",
      "2022-02-25 18:51:47.198700: Epoch: 13, Batch: 35, Loss: 0.5047, Elapsed: 0m1s\n",
      "2022-02-25 18:51:48.462811: Epoch: 13, Batch: 36, Loss: 0.5461, Elapsed: 0m1s\n",
      "2022-02-25 18:51:50.190897: Epoch: 13, Batch: 37, Loss: 0.5430, Elapsed: 0m1s\n",
      "2022-02-25 18:51:51.706774: Epoch: 13, Batch: 38, Loss: 0.5364, Elapsed: 0m1s\n",
      "2022-02-25 18:51:53.657098: Epoch: 13, Batch: 39, Loss: 0.5045, Elapsed: 0m1s\n",
      "2022-02-25 18:51:55.188704: Epoch: 13, Batch: 40, Loss: 0.5255, Elapsed: 0m1s\n",
      "2022-02-25 18:51:57.508312: Epoch: 13, Batch: 41, Loss: 0.4946, Elapsed: 0m2s\n",
      "2022-02-25 18:51:58.939752: Epoch: 13, Batch: 42, Loss: 0.5202, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:52:00.312508: Epoch: 13, Batch: 43, Loss: 0.5472, Elapsed: 0m1s\n",
      "2022-02-25 18:52:02.932459: Epoch: 13, Batch: 44, Loss: 0.4649, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:52:04.518011: Epoch: 13, Batch: 45, Loss: 0.5024, Elapsed: 0m1s\n",
      "2022-02-25 18:52:06.669615: Epoch: 13, Batch: 46, Loss: 0.4571, Elapsed: 0m1s\n",
      "2022-02-25 18:52:07.914504: Epoch: 13, Batch: 47, Loss: 0.5494, Elapsed: 0m1s\n",
      "2022-02-25 18:52:09.212022: Epoch: 13, Batch: 48, Loss: 0.5634, Elapsed: 0m1s\n",
      "2022-02-25 18:52:10.746935: Epoch: 13, Batch: 49, Loss: 0.5456, Elapsed: 0m1s\n",
      "2022-02-25 18:52:12.490155: Epoch: 13, Batch: 50, Loss: 0.5170, Elapsed: 0m1s\n",
      "2022-02-25 18:52:12.701677 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:52:52.656208: validation Test:  Loss: 0.5036,  AUC: 0.6513, Acc: 78.1527,  Precision: 0.6919 -- Elapsed: 0m39s\n",
      "2022-02-25 18:52:52.656276 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:53:31.019446: training Test:  Loss: 0.5150,  AUC: 0.6461, Acc: 77.4285,  Precision: 0.7234 -- Elapsed: 0m38s\n",
      "2022-02-25 18:53:31.899915: Epoch: 14, Batch: 1, Loss: 0.5560, Elapsed: 0m0s\n",
      "2022-02-25 18:53:33.002794: Epoch: 14, Batch: 2, Loss: 0.5607, Elapsed: 0m0s\n",
      "2022-02-25 18:53:34.692847: Epoch: 14, Batch: 3, Loss: 0.4971, Elapsed: 0m1s\n",
      "2022-02-25 18:53:37.147161: Epoch: 14, Batch: 4, Loss: 0.4663, Elapsed: 0m2s\n",
      "2022-02-25 18:53:38.897807: Epoch: 14, Batch: 5, Loss: 0.5257, Elapsed: 0m1s\n",
      "2022-02-25 18:53:40.566400: Epoch: 14, Batch: 6, Loss: 0.5319, Elapsed: 0m1s\n",
      "2022-02-25 18:53:41.583557: Epoch: 14, Batch: 7, Loss: 0.5892, Elapsed: 0m0s\n",
      "2022-02-25 18:53:43.132504: Epoch: 14, Batch: 8, Loss: 0.5035, Elapsed: 0m1s\n",
      "2022-02-25 18:53:44.544879: Epoch: 14, Batch: 9, Loss: 0.5486, Elapsed: 0m1s\n",
      "2022-02-25 18:53:46.227535: Epoch: 14, Batch: 10, Loss: 0.5040, Elapsed: 0m1s\n",
      "2022-02-25 18:53:47.813561: Epoch: 14, Batch: 11, Loss: 0.5458, Elapsed: 0m1s\n",
      "2022-02-25 18:53:49.722735: Epoch: 14, Batch: 12, Loss: 0.5093, Elapsed: 0m1s\n",
      "2022-02-25 18:53:51.106336: Epoch: 14, Batch: 13, Loss: 0.5527, Elapsed: 0m1s\n",
      "2022-02-25 18:53:52.164960: Epoch: 14, Batch: 14, Loss: 0.5783, Elapsed: 0m0s\n",
      "2022-02-25 18:53:54.318067: Epoch: 14, Batch: 15, Loss: 0.4561, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:53:55.524978: Epoch: 14, Batch: 16, Loss: 0.5571, Elapsed: 0m0s\n",
      "2022-02-25 18:53:57.267061: Epoch: 14, Batch: 17, Loss: 0.5172, Elapsed: 0m1s\n",
      "2022-02-25 18:53:58.594984: Epoch: 14, Batch: 18, Loss: 0.5658, Elapsed: 0m1s\n",
      "2022-02-25 18:54:00.210023: Epoch: 14, Batch: 19, Loss: 0.5047, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:54:01.563940: Epoch: 14, Batch: 20, Loss: 0.5570, Elapsed: 0m1s\n",
      "2022-02-25 18:54:03.022267: Epoch: 14, Batch: 21, Loss: 0.5392, Elapsed: 0m1s\n",
      "2022-02-25 18:54:04.633324: Epoch: 14, Batch: 22, Loss: 0.5001, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:54:06.129575: Epoch: 14, Batch: 23, Loss: 0.5352, Elapsed: 0m1s\n",
      "2022-02-25 18:54:08.412529: Epoch: 14, Batch: 24, Loss: 0.4215, Elapsed: 0m2s\n",
      "2022-02-25 18:54:09.835465: Epoch: 14, Batch: 25, Loss: 0.5523, Elapsed: 0m1s\n",
      "2022-02-25 18:54:11.573041: Epoch: 14, Batch: 26, Loss: 0.5344, Elapsed: 0m1s\n",
      "2022-02-25 18:54:12.980071: Epoch: 14, Batch: 27, Loss: 0.5416, Elapsed: 0m1s\n",
      "2022-02-25 18:54:14.887754: Epoch: 14, Batch: 28, Loss: 0.5010, Elapsed: 0m1s\n",
      "2022-02-25 18:54:16.553177: Epoch: 14, Batch: 29, Loss: 0.5305, Elapsed: 0m1s\n",
      "2022-02-25 18:54:18.639755: Epoch: 14, Batch: 30, Loss: 0.4925, Elapsed: 0m1s\n",
      "2022-02-25 18:54:20.900324: Epoch: 14, Batch: 31, Loss: 0.4457, Elapsed: 0m2s\n",
      "2022-02-25 18:54:23.180794: Epoch: 14, Batch: 32, Loss: 0.4929, Elapsed: 0m2s\n",
      "2022-02-25 18:54:24.469745: Epoch: 14, Batch: 33, Loss: 0.5447, Elapsed: 0m1s\n",
      "2022-02-25 18:54:26.377563: Epoch: 14, Batch: 34, Loss: 0.4960, Elapsed: 0m1s\n",
      "2022-02-25 18:54:27.575003: Epoch: 14, Batch: 35, Loss: 0.5663, Elapsed: 0m0s\n",
      "2022-02-25 18:54:29.016450: Epoch: 14, Batch: 36, Loss: 0.4900, Elapsed: 0m1s\n",
      "2022-02-25 18:54:30.552896: Epoch: 14, Batch: 37, Loss: 0.5242, Elapsed: 0m1s\n",
      "2022-02-25 18:54:32.123310: Epoch: 14, Batch: 38, Loss: 0.5399, Elapsed: 0m1s\n",
      "2022-02-25 18:54:33.928871: Epoch: 14, Batch: 39, Loss: 0.4834, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:54:35.175574: Epoch: 14, Batch: 40, Loss: 0.5470, Elapsed: 0m1s\n",
      "2022-02-25 18:54:36.618883: Epoch: 14, Batch: 41, Loss: 0.5165, Elapsed: 0m1s\n",
      "2022-02-25 18:54:38.012853: Epoch: 14, Batch: 42, Loss: 0.4832, Elapsed: 0m1s\n",
      "2022-02-25 18:54:39.886527: Epoch: 14, Batch: 43, Loss: 0.4822, Elapsed: 0m1s\n",
      "2022-02-25 18:54:41.129885: Epoch: 14, Batch: 44, Loss: 0.5408, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:54:42.978970: Epoch: 14, Batch: 45, Loss: 0.5187, Elapsed: 0m1s\n",
      "2022-02-25 18:54:44.432016: Epoch: 14, Batch: 46, Loss: 0.5292, Elapsed: 0m1s\n",
      "2022-02-25 18:54:45.788847: Epoch: 14, Batch: 47, Loss: 0.5299, Elapsed: 0m1s\n",
      "2022-02-25 18:54:47.255801: Epoch: 14, Batch: 48, Loss: 0.5639, Elapsed: 0m1s\n",
      "2022-02-25 18:54:48.677119: Epoch: 14, Batch: 49, Loss: 0.5394, Elapsed: 0m1s\n",
      "2022-02-25 18:54:49.674547: Epoch: 14, Batch: 50, Loss: 0.5650, Elapsed: 0m0s\n",
      "2022-02-25 18:54:49.876119 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:55:29.917660: validation Test:  Loss: 0.5028,  AUC: 0.6607, Acc: 77.8793,  Precision: 0.5684 -- Elapsed: 0m40s\n",
      "2022-02-25 18:55:29.917730 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:56:07.394532: training Test:  Loss: 0.5137,  AUC: 0.6562, Acc: 77.1872,  Precision: 0.5854 -- Elapsed: 0m37s\n",
      "2022-02-25 18:56:08.446214: Epoch: 15, Batch: 1, Loss: 0.5568, Elapsed: 0m1s\n",
      "2022-02-25 18:56:10.761440: Epoch: 15, Batch: 2, Loss: 0.4435, Elapsed: 0m2s\n",
      "2022-02-25 18:56:12.686494: Epoch: 15, Batch: 3, Loss: 0.4934, Elapsed: 0m1s\n",
      "2022-02-25 18:56:14.388462: Epoch: 15, Batch: 4, Loss: 0.5212, Elapsed: 0m1s\n",
      "2022-02-25 18:56:16.239001: Epoch: 15, Batch: 5, Loss: 0.5004, Elapsed: 0m1s\n",
      "2022-02-25 18:56:17.679533: Epoch: 15, Batch: 6, Loss: 0.5286, Elapsed: 0m1s\n",
      "2022-02-25 18:56:19.276371: Epoch: 15, Batch: 7, Loss: 0.5140, Elapsed: 0m1s\n",
      "2022-02-25 18:56:20.659255: Epoch: 15, Batch: 8, Loss: 0.5489, Elapsed: 0m1s\n",
      "2022-02-25 18:56:22.108404: Epoch: 15, Batch: 9, Loss: 0.5353, Elapsed: 0m1s\n",
      "2022-02-25 18:56:23.940643: Epoch: 15, Batch: 10, Loss: 0.5087, Elapsed: 0m1s\n",
      "2022-02-25 18:56:25.205575: Epoch: 15, Batch: 11, Loss: 0.5577, Elapsed: 0m1s\n",
      "2022-02-25 18:56:26.390644: Epoch: 15, Batch: 12, Loss: 0.5410, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:56:28.202754: Epoch: 15, Batch: 13, Loss: 0.5165, Elapsed: 0m1s\n",
      "2022-02-25 18:56:29.498141: Epoch: 15, Batch: 14, Loss: 0.4841, Elapsed: 0m1s\n",
      "2022-02-25 18:56:30.869955: Epoch: 15, Batch: 15, Loss: 0.5417, Elapsed: 0m1s\n",
      "2022-02-25 18:56:32.530453: Epoch: 15, Batch: 16, Loss: 0.4966, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:56:34.159001: Epoch: 15, Batch: 17, Loss: 0.5302, Elapsed: 0m1s\n",
      "2022-02-25 18:56:35.554144: Epoch: 15, Batch: 18, Loss: 0.5228, Elapsed: 0m1s\n",
      "2022-02-25 18:56:37.142458: Epoch: 15, Batch: 19, Loss: 0.4995, Elapsed: 0m1s\n",
      "2022-02-25 18:56:38.542637: Epoch: 15, Batch: 20, Loss: 0.5542, Elapsed: 0m1s\n",
      "2022-02-25 18:56:39.990931: Epoch: 15, Batch: 21, Loss: 0.5623, Elapsed: 0m1s\n",
      "2022-02-25 18:56:41.227101: Epoch: 15, Batch: 22, Loss: 0.5456, Elapsed: 0m1s\n",
      "2022-02-25 18:56:42.501778: Epoch: 15, Batch: 23, Loss: 0.5414, Elapsed: 0m1s\n",
      "2022-02-25 18:56:43.812583: Epoch: 15, Batch: 24, Loss: 0.5281, Elapsed: 0m1s\n",
      "2022-02-25 18:56:45.200610: Epoch: 15, Batch: 25, Loss: 0.5386, Elapsed: 0m1s\n",
      "2022-02-25 18:56:46.650648: Epoch: 15, Batch: 26, Loss: 0.4920, Elapsed: 0m1s\n",
      "2022-02-25 18:56:49.050550: Epoch: 15, Batch: 27, Loss: 0.4897, Elapsed: 0m2s\n",
      "2022-02-25 18:56:50.140738: Epoch: 15, Batch: 28, Loss: 0.5614, Elapsed: 0m0s\n",
      "2022-02-25 18:56:51.138800: Epoch: 15, Batch: 29, Loss: 0.5971, Elapsed: 0m0s\n",
      "2022-02-25 18:56:53.052093: Epoch: 15, Batch: 30, Loss: 0.4833, Elapsed: 0m1s\n",
      "2022-02-25 18:56:54.666868: Epoch: 15, Batch: 31, Loss: 0.4993, Elapsed: 0m1s\n",
      "2022-02-25 18:56:56.049946: Epoch: 15, Batch: 32, Loss: 0.5388, Elapsed: 0m1s\n",
      "2022-02-25 18:56:57.437527: Epoch: 15, Batch: 33, Loss: 0.5408, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:56:59.934001: Epoch: 15, Batch: 34, Loss: 0.4629, Elapsed: 0m2s\n",
      "2022-02-25 18:57:01.959408: Epoch: 15, Batch: 35, Loss: 0.4918, Elapsed: 0m1s\n",
      "2022-02-25 18:57:03.500157: Epoch: 15, Batch: 36, Loss: 0.5015, Elapsed: 0m1s\n",
      "2022-02-25 18:57:05.147043: Epoch: 15, Batch: 37, Loss: 0.5319, Elapsed: 0m1s\n",
      "2022-02-25 18:57:06.515710: Epoch: 15, Batch: 38, Loss: 0.5471, Elapsed: 0m1s\n",
      "2022-02-25 18:57:08.105065: Epoch: 15, Batch: 39, Loss: 0.5315, Elapsed: 0m1s\n",
      "2022-02-25 18:57:09.079587: Epoch: 15, Batch: 40, Loss: 0.5641, Elapsed: 0m0s\n",
      "2022-02-25 18:57:11.259306: Epoch: 15, Batch: 41, Loss: 0.4198, Elapsed: 0m1s\n",
      "2022-02-25 18:57:12.714461: Epoch: 15, Batch: 42, Loss: 0.5145, Elapsed: 0m1s\n",
      "2022-02-25 18:57:14.232569: Epoch: 15, Batch: 43, Loss: 0.5075, Elapsed: 0m1s\n",
      "2022-02-25 18:57:15.326631: Epoch: 15, Batch: 44, Loss: 0.5725, Elapsed: 0m0s\n",
      "2022-02-25 18:57:16.649432: Epoch: 15, Batch: 45, Loss: 0.5522, Elapsed: 0m1s\n",
      "2022-02-25 18:57:18.760155: Epoch: 15, Batch: 46, Loss: 0.4548, Elapsed: 0m1s\n",
      "2022-02-25 18:57:19.901464: Epoch: 15, Batch: 47, Loss: 0.5684, Elapsed: 0m0s\n",
      "2022-02-25 18:57:21.064990: Epoch: 15, Batch: 48, Loss: 0.5554, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:57:22.129549: Epoch: 15, Batch: 49, Loss: 0.5568, Elapsed: 0m0s\n",
      "2022-02-25 18:57:23.849621: Epoch: 15, Batch: 50, Loss: 0.4832, Elapsed: 0m1s\n",
      "2022-02-25 18:57:24.064276 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 18:58:03.164471: validation Test:  Loss: 0.5005,  AUC: 0.6597, Acc: 78.3441,  Precision: 0.6583 -- Elapsed: 0m39s\n",
      "2022-02-25 18:58:03.164533 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 18:58:40.602807: training Test:  Loss: 0.5116,  AUC: 0.6554, Acc: 77.4213,  Precision: 0.6610 -- Elapsed: 0m37s\n",
      "2022-02-25 18:58:41.702737: Epoch: 16, Batch: 1, Loss: 0.5275, Elapsed: 0m1s\n",
      "2022-02-25 18:58:43.048022: Epoch: 16, Batch: 2, Loss: 0.5465, Elapsed: 0m1s\n",
      "2022-02-25 18:58:44.358609: Epoch: 16, Batch: 3, Loss: 0.5380, Elapsed: 0m1s\n",
      "2022-02-25 18:58:45.931691: Epoch: 16, Batch: 4, Loss: 0.5009, Elapsed: 0m1s\n",
      "2022-02-25 18:58:46.965352: Epoch: 16, Batch: 5, Loss: 0.5756, Elapsed: 0m0s\n",
      "2022-02-25 18:58:49.089742: Epoch: 16, Batch: 6, Loss: 0.4175, Elapsed: 0m1s\n",
      "2022-02-25 18:58:50.438775: Epoch: 16, Batch: 7, Loss: 0.4836, Elapsed: 0m1s\n",
      "2022-02-25 18:58:52.671963: Epoch: 16, Batch: 8, Loss: 0.4403, Elapsed: 0m2s\n",
      "2022-02-25 18:58:53.839289: Epoch: 16, Batch: 9, Loss: 0.5578, Elapsed: 0m0s\n",
      "2022-02-25 18:58:55.669495: Epoch: 16, Batch: 10, Loss: 0.4830, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 18:58:57.424759: Epoch: 16, Batch: 11, Loss: 0.4853, Elapsed: 0m1s\n",
      "2022-02-25 18:58:58.675658: Epoch: 16, Batch: 12, Loss: 0.5530, Elapsed: 0m1s\n",
      "2022-02-25 18:59:00.072152: Epoch: 16, Batch: 13, Loss: 0.5237, Elapsed: 0m1s\n",
      "2022-02-25 18:59:01.704863: Epoch: 16, Batch: 14, Loss: 0.5338, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:03.396363: Epoch: 16, Batch: 15, Loss: 0.5352, Elapsed: 0m1s\n",
      "2022-02-25 18:59:04.975798: Epoch: 16, Batch: 16, Loss: 0.4865, Elapsed: 0m1s\n",
      "2022-02-25 18:59:06.227147: Epoch: 16, Batch: 17, Loss: 0.5471, Elapsed: 0m1s\n",
      "2022-02-25 18:59:07.914773: Epoch: 16, Batch: 18, Loss: 0.5149, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:09.197117: Epoch: 16, Batch: 19, Loss: 0.5505, Elapsed: 0m1s\n",
      "2022-02-25 18:59:11.435826: Epoch: 16, Batch: 20, Loss: 0.4906, Elapsed: 0m2s\n",
      "2022-02-25 18:59:12.497301: Epoch: 16, Batch: 21, Loss: 0.5566, Elapsed: 0m0s\n",
      "2022-02-25 18:59:14.085358: Epoch: 16, Batch: 22, Loss: 0.5023, Elapsed: 0m1s\n",
      "2022-02-25 18:59:15.087120: Epoch: 16, Batch: 23, Loss: 0.6097, Elapsed: 0m0s\n",
      "2022-02-25 18:59:16.278103: Epoch: 16, Batch: 24, Loss: 0.5387, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:18.189665: Epoch: 16, Batch: 25, Loss: 0.5031, Elapsed: 0m1s\n",
      "2022-02-25 18:59:20.679859: Epoch: 16, Batch: 26, Loss: 0.4662, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:22.151610: Epoch: 16, Batch: 27, Loss: 0.5332, Elapsed: 0m1s\n",
      "2022-02-25 18:59:23.616221: Epoch: 16, Batch: 28, Loss: 0.5487, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:25.288502: Epoch: 16, Batch: 29, Loss: 0.4981, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:27.117819: Epoch: 16, Batch: 30, Loss: 0.5220, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 18:59:28.525388: Epoch: 16, Batch: 31, Loss: 0.5391, Elapsed: 0m1s\n",
      "2022-02-25 18:59:29.999766: Epoch: 16, Batch: 32, Loss: 0.5639, Elapsed: 0m1s\n",
      "2022-02-25 18:59:31.257459: Epoch: 16, Batch: 33, Loss: 0.5396, Elapsed: 0m1s\n",
      "2022-02-25 18:59:32.703562: Epoch: 16, Batch: 34, Loss: 0.5503, Elapsed: 0m1s\n",
      "2022-02-25 18:59:34.247732: Epoch: 16, Batch: 35, Loss: 0.5135, Elapsed: 0m1s\n",
      "2022-02-25 18:59:35.673834: Epoch: 16, Batch: 36, Loss: 0.5351, Elapsed: 0m1s\n",
      "2022-02-25 18:59:36.798116: Epoch: 16, Batch: 37, Loss: 0.5634, Elapsed: 0m0s\n",
      "2022-02-25 18:59:38.720806: Epoch: 16, Batch: 38, Loss: 0.5148, Elapsed: 0m1s\n",
      "2022-02-25 18:59:40.690652: Epoch: 16, Batch: 39, Loss: 0.4950, Elapsed: 0m1s\n",
      "2022-02-25 18:59:42.798098: Epoch: 16, Batch: 40, Loss: 0.4579, Elapsed: 0m1s\n",
      "2022-02-25 18:59:43.830209: Epoch: 16, Batch: 41, Loss: 0.5758, Elapsed: 0m0s\n",
      "2022-02-25 18:59:44.970341: Epoch: 16, Batch: 42, Loss: 0.5711, Elapsed: 0m0s\n",
      "2022-02-25 18:59:46.995627: Epoch: 16, Batch: 43, Loss: 0.4922, Elapsed: 0m1s\n",
      "2022-02-25 18:59:48.388510: Epoch: 16, Batch: 44, Loss: 0.5543, Elapsed: 0m1s\n",
      "2022-02-25 18:59:50.009309: Epoch: 16, Batch: 45, Loss: 0.5342, Elapsed: 0m1s\n",
      "2022-02-25 18:59:51.381379: Epoch: 16, Batch: 46, Loss: 0.5455, Elapsed: 0m1s\n",
      "2022-02-25 18:59:52.826695: Epoch: 16, Batch: 47, Loss: 0.5218, Elapsed: 0m1s\n",
      "2022-02-25 18:59:54.436927: Epoch: 16, Batch: 48, Loss: 0.5393, Elapsed: 0m1s\n",
      "2022-02-25 18:59:55.975347: Epoch: 16, Batch: 49, Loss: 0.5059, Elapsed: 0m1s\n",
      "2022-02-25 18:59:57.670429: Epoch: 16, Batch: 50, Loss: 0.5224, Elapsed: 0m1s\n",
      "2022-02-25 18:59:57.894152 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 19:00:37.741616: validation Test:  Loss: 0.5006,  AUC: 0.6601, Acc: 78.2518,  Precision: 0.6094 -- Elapsed: 0m39s\n",
      "2022-02-25 19:00:37.741691 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 19:01:16.997315: training Test:  Loss: 0.5119,  AUC: 0.6545, Acc: 77.3294,  Precision: 0.6090 -- Elapsed: 0m39s\n",
      "2022-02-25 19:01:18.096062: Epoch: 17, Batch: 1, Loss: 0.5465, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 19:01:19.294771: Epoch: 17, Batch: 2, Loss: 0.5517, Elapsed: 0m0s\n",
      "2022-02-25 19:01:20.881375: Epoch: 17, Batch: 3, Loss: 0.5621, Elapsed: 0m1s\n",
      "2022-02-25 19:01:22.364290: Epoch: 17, Batch: 4, Loss: 0.5423, Elapsed: 0m1s\n",
      "2022-02-25 19:01:24.895332: Epoch: 17, Batch: 5, Loss: 0.4452, Elapsed: 0m2s\n",
      "2022-02-25 19:01:26.321132: Epoch: 17, Batch: 6, Loss: 0.5565, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:27.782080: Epoch: 17, Batch: 7, Loss: 0.5485, Elapsed: 0m1s\n",
      "2022-02-25 19:01:29.486884: Epoch: 17, Batch: 8, Loss: 0.4996, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:31.789892: Epoch: 17, Batch: 9, Loss: 0.4171, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:33.013117: Epoch: 17, Batch: 10, Loss: 0.5646, Elapsed: 0m1s\n",
      "2022-02-25 19:01:34.769358: Epoch: 17, Batch: 11, Loss: 0.5216, Elapsed: 0m1s\n",
      "2022-02-25 19:01:37.019113: Epoch: 17, Batch: 12, Loss: 0.4577, Elapsed: 0m2s\n",
      "2022-02-25 19:01:38.812080: Epoch: 17, Batch: 13, Loss: 0.5311, Elapsed: 0m1s\n",
      "2022-02-25 19:01:40.556256: Epoch: 17, Batch: 14, Loss: 0.5340, Elapsed: 0m1s\n",
      "2022-02-25 19:01:42.263053: Epoch: 17, Batch: 15, Loss: 0.5010, Elapsed: 0m1s\n",
      "2022-02-25 19:01:43.424587: Epoch: 17, Batch: 16, Loss: 0.5574, Elapsed: 0m0s\n",
      "2022-02-25 19:01:44.875073: Epoch: 17, Batch: 17, Loss: 0.5414, Elapsed: 0m1s\n",
      "2022-02-25 19:01:45.964438: Epoch: 17, Batch: 18, Loss: 0.5732, Elapsed: 0m0s\n",
      "2022-02-25 19:01:47.466116: Epoch: 17, Batch: 19, Loss: 0.5385, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:49.940368: Epoch: 17, Batch: 20, Loss: 0.4651, Elapsed: 0m2s\n",
      "2022-02-25 19:01:51.474653: Epoch: 17, Batch: 21, Loss: 0.5502, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:52.891715: Epoch: 17, Batch: 22, Loss: 0.5281, Elapsed: 0m1s\n",
      "2022-02-25 19:01:54.892894: Epoch: 17, Batch: 23, Loss: 0.4831, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:56.469580: Epoch: 17, Batch: 24, Loss: 0.5366, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:01:58.503357: Epoch: 17, Batch: 25, Loss: 0.4963, Elapsed: 0m1s\n",
      "2022-02-25 19:01:59.585789: Epoch: 17, Batch: 26, Loss: 0.5919, Elapsed: 0m0s\n",
      "2022-02-25 19:02:01.785552: Epoch: 17, Batch: 27, Loss: 0.4930, Elapsed: 0m1s\n",
      "2022-02-25 19:02:03.469536: Epoch: 17, Batch: 28, Loss: 0.5143, Elapsed: 0m1s\n",
      "2022-02-25 19:02:04.959228: Epoch: 17, Batch: 29, Loss: 0.5172, Elapsed: 0m1s\n",
      "2022-02-25 19:02:06.261647: Epoch: 17, Batch: 30, Loss: 0.5536, Elapsed: 0m1s\n",
      "2022-02-25 19:02:07.608457: Epoch: 17, Batch: 31, Loss: 0.5546, Elapsed: 0m1s\n",
      "2022-02-25 19:02:09.085307: Epoch: 17, Batch: 32, Loss: 0.5379, Elapsed: 0m1s\n",
      "2022-02-25 19:02:10.742474: Epoch: 17, Batch: 33, Loss: 0.5297, Elapsed: 0m1s\n",
      "2022-02-25 19:02:12.421616: Epoch: 17, Batch: 34, Loss: 0.5008, Elapsed: 0m1s\n",
      "2022-02-25 19:02:13.666389: Epoch: 17, Batch: 35, Loss: 0.5377, Elapsed: 0m1s\n",
      "2022-02-25 19:02:15.013310: Epoch: 17, Batch: 36, Loss: 0.5559, Elapsed: 0m1s\n",
      "2022-02-25 19:02:16.667157: Epoch: 17, Batch: 37, Loss: 0.5034, Elapsed: 0m1s\n",
      "2022-02-25 19:02:19.062674: Epoch: 17, Batch: 38, Loss: 0.4911, Elapsed: 0m2s\n",
      "2022-02-25 19:02:20.799047: Epoch: 17, Batch: 39, Loss: 0.4942, Elapsed: 0m1s\n",
      "2022-02-25 19:02:22.723803: Epoch: 17, Batch: 40, Loss: 0.5006, Elapsed: 0m1s\n",
      "2022-02-25 19:02:24.237046: Epoch: 17, Batch: 41, Loss: 0.5297, Elapsed: 0m1s\n",
      "2022-02-25 19:02:25.651971: Epoch: 17, Batch: 42, Loss: 0.5386, Elapsed: 0m1s\n",
      "2022-02-25 19:02:27.660411: Epoch: 17, Batch: 43, Loss: 0.5090, Elapsed: 0m1s\n",
      "2022-02-25 19:02:28.694906: Epoch: 17, Batch: 44, Loss: 0.5617, Elapsed: 0m0s\n",
      "2022-02-25 19:02:30.167148: Epoch: 17, Batch: 45, Loss: 0.4922, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:02:31.310233: Epoch: 17, Batch: 46, Loss: 0.5571, Elapsed: 0m0s\n",
      "2022-02-25 19:02:33.226134: Epoch: 17, Batch: 47, Loss: 0.5139, Elapsed: 0m1s\n",
      "2022-02-25 19:02:34.810622: Epoch: 17, Batch: 48, Loss: 0.5234, Elapsed: 0m1s\n",
      "2022-02-25 19:02:36.231133: Epoch: 17, Batch: 49, Loss: 0.4783, Elapsed: 0m1s\n",
      "2022-02-25 19:02:38.082503: Epoch: 17, Batch: 50, Loss: 0.4884, Elapsed: 0m1s\n",
      "2022-02-25 19:02:38.299929 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 19:03:19.437397: validation Test:  Loss: 0.5059,  AUC: 0.6621, Acc: 77.6641,  Precision: 0.5408 -- Elapsed: 0m41s\n",
      "2022-02-25 19:03:19.437468 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 19:03:59.063362: training Test:  Loss: 0.5159,  AUC: 0.6576, Acc: 76.9459,  Precision: 0.5521 -- Elapsed: 0m39s\n",
      "2022-02-25 19:03:59.953244: Epoch: 18, Batch: 1, Loss: 0.5584, Elapsed: 0m0s\n",
      "2022-02-25 19:04:01.594068: Epoch: 18, Batch: 2, Loss: 0.5031, Elapsed: 0m1s\n",
      "2022-02-25 19:04:03.297102: Epoch: 18, Batch: 3, Loss: 0.4935, Elapsed: 0m1s\n",
      "2022-02-25 19:04:04.827241: Epoch: 18, Batch: 4, Loss: 0.5014, Elapsed: 0m1s\n",
      "2022-02-25 19:04:06.273665: Epoch: 18, Batch: 5, Loss: 0.5447, Elapsed: 0m1s\n",
      "2022-02-25 19:04:08.305985: Epoch: 18, Batch: 6, Loss: 0.5042, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 19:04:09.985884: Epoch: 18, Batch: 7, Loss: 0.5005, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:04:11.160831: Epoch: 18, Batch: 8, Loss: 0.5728, Elapsed: 0m0s\n",
      "2022-02-25 19:04:12.620074: Epoch: 18, Batch: 9, Loss: 0.4950, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:04:14.589015: Epoch: 18, Batch: 10, Loss: 0.5084, Elapsed: 0m1s\n",
      "2022-02-25 19:04:16.054748: Epoch: 18, Batch: 11, Loss: 0.5425, Elapsed: 0m1s\n",
      "2022-02-25 19:04:17.535065: Epoch: 18, Batch: 12, Loss: 0.5396, Elapsed: 0m1s\n",
      "2022-02-25 19:04:18.968287: Epoch: 18, Batch: 13, Loss: 0.5319, Elapsed: 0m1s\n",
      "2022-02-25 19:04:20.835244: Epoch: 18, Batch: 14, Loss: 0.4848, Elapsed: 0m1s\n",
      "2022-02-25 19:04:22.031911: Epoch: 18, Batch: 15, Loss: 0.5638, Elapsed: 0m0s\n",
      "2022-02-25 19:04:23.636911: Epoch: 18, Batch: 16, Loss: 0.5323, Elapsed: 0m1s\n",
      "2022-02-25 19:04:26.105808: Epoch: 18, Batch: 17, Loss: 0.4645, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 19:04:27.673657: Epoch: 18, Batch: 18, Loss: 0.5323, Elapsed: 0m1s\n",
      "2022-02-25 19:04:29.018792: Epoch: 18, Batch: 19, Loss: 0.4799, Elapsed: 0m1s\n",
      "2022-02-25 19:04:31.203982: Epoch: 18, Batch: 20, Loss: 0.4588, Elapsed: 0m1s\n",
      "2022-02-25 19:04:32.943743: Epoch: 18, Batch: 21, Loss: 0.5298, Elapsed: 0m1s\n",
      "2022-02-25 19:04:34.363488: Epoch: 18, Batch: 22, Loss: 0.5549, Elapsed: 0m1s\n",
      "2022-02-25 19:04:36.490399: Epoch: 18, Batch: 23, Loss: 0.4912, Elapsed: 0m1s\n",
      "2022-02-25 19:04:38.018152: Epoch: 18, Batch: 24, Loss: 0.5377, Elapsed: 0m1s\n",
      "2022-02-25 19:04:39.319899: Epoch: 18, Batch: 25, Loss: 0.5435, Elapsed: 0m1s\n",
      "2022-02-25 19:04:40.560269: Epoch: 18, Batch: 26, Loss: 0.5341, Elapsed: 0m1s\n",
      "2022-02-25 19:04:42.926352: Epoch: 18, Batch: 27, Loss: 0.4899, Elapsed: 0m2s\n",
      "2022-02-25 19:04:45.359036: Epoch: 18, Batch: 28, Loss: 0.4418, Elapsed: 0m2s\n",
      "2022-02-25 19:04:47.113962: Epoch: 18, Batch: 29, Loss: 0.5330, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:04:48.687560: Epoch: 18, Batch: 30, Loss: 0.5224, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:04:50.558202: Epoch: 18, Batch: 31, Loss: 0.5169, Elapsed: 0m1s\n",
      "2022-02-25 19:04:51.682601: Epoch: 18, Batch: 32, Loss: 0.5682, Elapsed: 0m0s\n",
      "2022-02-25 19:04:52.655599: Epoch: 18, Batch: 33, Loss: 0.5617, Elapsed: 0m0s\n",
      "2022-02-25 19:04:54.182535: Epoch: 18, Batch: 34, Loss: 0.5484, Elapsed: 0m1s\n",
      "2022-02-25 19:04:55.629191: Epoch: 18, Batch: 35, Loss: 0.5373, Elapsed: 0m1s\n",
      "2022-02-25 19:04:56.707582: Epoch: 18, Batch: 36, Loss: 0.5911, Elapsed: 0m0s\n",
      "2022-02-25 19:04:58.172732: Epoch: 18, Batch: 37, Loss: 0.5469, Elapsed: 0m1s\n",
      "2022-02-25 19:04:59.945494: Epoch: 18, Batch: 38, Loss: 0.5217, Elapsed: 0m1s\n",
      "2022-02-25 19:05:01.640888: Epoch: 18, Batch: 39, Loss: 0.5029, Elapsed: 0m1s\n",
      "2022-02-25 19:05:03.704910: Epoch: 18, Batch: 40, Loss: 0.4938, Elapsed: 0m1s\n",
      "2022-02-25 19:05:05.902426: Epoch: 18, Batch: 41, Loss: 0.4178, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:05:07.402369: Epoch: 18, Batch: 42, Loss: 0.5134, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:05:08.789154: Epoch: 18, Batch: 43, Loss: 0.5508, Elapsed: 0m1s\n",
      "2022-02-25 19:05:10.303246: Epoch: 18, Batch: 44, Loss: 0.5627, Elapsed: 0m1s\n",
      "2022-02-25 19:05:12.038058: Epoch: 18, Batch: 45, Loss: 0.5152, Elapsed: 0m1s\n",
      "2022-02-25 19:05:14.013133: Epoch: 18, Batch: 46, Loss: 0.4822, Elapsed: 0m1s\n",
      "2022-02-25 19:05:15.345888: Epoch: 18, Batch: 47, Loss: 0.5545, Elapsed: 0m1s\n",
      "2022-02-25 19:05:16.870703: Epoch: 18, Batch: 48, Loss: 0.5236, Elapsed: 0m1s\n",
      "2022-02-25 19:05:18.175692: Epoch: 18, Batch: 49, Loss: 0.5400, Elapsed: 0m1s\n",
      "2022-02-25 19:05:19.442422: Epoch: 18, Batch: 50, Loss: 0.5527, Elapsed: 0m1s\n",
      "2022-02-25 19:05:19.651783 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 19:06:01.032652: validation Test:  Loss: 0.5000,  AUC: 0.6612, Acc: 78.2433,  Precision: 0.6423 -- Elapsed: 0m41s\n",
      "2022-02-25 19:06:01.032741 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 19:06:40.371657: training Test:  Loss: 0.5110,  AUC: 0.6577, Acc: 77.6319,  Precision: 0.6799 -- Elapsed: 0m39s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:06:41.262247: Epoch: 19, Batch: 1, Loss: 0.5919, Elapsed: 0m0s\n",
      "2022-02-25 19:06:42.347241: Epoch: 19, Batch: 2, Loss: 0.5640, Elapsed: 0m0s\n",
      "2022-02-25 19:06:44.009582: Epoch: 19, Batch: 3, Loss: 0.4975, Elapsed: 0m1s\n",
      "2022-02-25 19:06:45.714785: Epoch: 19, Batch: 4, Loss: 0.5305, Elapsed: 0m1s\n",
      "2022-02-25 19:06:47.209586: Epoch: 19, Batch: 5, Loss: 0.5380, Elapsed: 0m1s\n",
      "2022-02-25 19:06:48.911932: Epoch: 19, Batch: 6, Loss: 0.5295, Elapsed: 0m1s\n",
      "2022-02-25 19:06:50.447426: Epoch: 19, Batch: 7, Loss: 0.5386, Elapsed: 0m1s\n",
      "2022-02-25 19:06:51.841351: Epoch: 19, Batch: 8, Loss: 0.5257, Elapsed: 0m1s\n",
      "2022-02-25 19:06:53.533993: Epoch: 19, Batch: 9, Loss: 0.4948, Elapsed: 0m1s\n",
      "2022-02-25 19:06:55.490262: Epoch: 19, Batch: 10, Loss: 0.5081, Elapsed: 0m1s\n",
      "2022-02-25 19:06:58.016799: Epoch: 19, Batch: 11, Loss: 0.4628, Elapsed: 0m2s\n",
      "2022-02-25 19:06:59.964239: Epoch: 19, Batch: 12, Loss: 0.4925, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:01.321962: Epoch: 19, Batch: 13, Loss: 0.5542, Elapsed: 0m1s\n",
      "2022-02-25 19:07:03.543522: Epoch: 19, Batch: 14, Loss: 0.4193, Elapsed: 0m2s\n",
      "2022-02-25 19:07:04.995446: Epoch: 19, Batch: 15, Loss: 0.5380, Elapsed: 0m1s\n",
      "2022-02-25 19:07:06.299571: Epoch: 19, Batch: 16, Loss: 0.5408, Elapsed: 0m1s\n",
      "2022-02-25 19:07:07.813233: Epoch: 19, Batch: 17, Loss: 0.5149, Elapsed: 0m1s\n",
      "2022-02-25 19:07:09.029445: Epoch: 19, Batch: 18, Loss: 0.5536, Elapsed: 0m0s\n",
      "2022-02-25 19:07:10.656640: Epoch: 19, Batch: 19, Loss: 0.5295, Elapsed: 0m1s\n",
      "2022-02-25 19:07:12.266681: Epoch: 19, Batch: 20, Loss: 0.5217, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:13.742333: Epoch: 19, Batch: 21, Loss: 0.5376, Elapsed: 0m1s\n",
      "2022-02-25 19:07:15.282624: Epoch: 19, Batch: 22, Loss: 0.5232, Elapsed: 0m1s\n",
      "2022-02-25 19:07:16.999543: Epoch: 19, Batch: 23, Loss: 0.4965, Elapsed: 0m1s\n",
      "2022-02-25 19:07:18.082965: Epoch: 19, Batch: 24, Loss: 0.5629, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:19.810312: Epoch: 19, Batch: 25, Loss: 0.5157, Elapsed: 0m1s\n",
      "2022-02-25 19:07:22.184767: Epoch: 19, Batch: 26, Loss: 0.4440, Elapsed: 0m2s\n",
      "2022-02-25 19:07:23.706434: Epoch: 19, Batch: 27, Loss: 0.5530, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:25.247430: Epoch: 19, Batch: 28, Loss: 0.5384, Elapsed: 0m1s\n",
      "2022-02-25 19:07:26.739507: Epoch: 19, Batch: 29, Loss: 0.4805, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:27.853241: Epoch: 19, Batch: 30, Loss: 0.5555, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:29.787602: Epoch: 19, Batch: 31, Loss: 0.5146, Elapsed: 0m1s\n",
      "2022-02-25 19:07:31.335770: Epoch: 19, Batch: 32, Loss: 0.4930, Elapsed: 0m1s\n",
      "2022-02-25 19:07:32.480793: Epoch: 19, Batch: 33, Loss: 0.5745, Elapsed: 0m0s\n",
      "2022-02-25 19:07:33.981588: Epoch: 19, Batch: 34, Loss: 0.5579, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:36.309856: Epoch: 19, Batch: 35, Loss: 0.4921, Elapsed: 0m2s\n",
      "2022-02-25 19:07:38.397992: Epoch: 19, Batch: 36, Loss: 0.4910, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:40.214247: Epoch: 19, Batch: 37, Loss: 0.4914, Elapsed: 0m1s\n",
      "2022-02-25 19:07:42.203035: Epoch: 19, Batch: 38, Loss: 0.5060, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:43.575379: Epoch: 19, Batch: 39, Loss: 0.5788, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:45.327463: Epoch: 19, Batch: 40, Loss: 0.5234, Elapsed: 0m1s\n",
      "2022-02-25 19:07:46.754982: Epoch: 19, Batch: 41, Loss: 0.5459, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:07:47.947647: Epoch: 19, Batch: 42, Loss: 0.5746, Elapsed: 0m0s\n",
      "2022-02-25 19:07:49.248255: Epoch: 19, Batch: 43, Loss: 0.5342, Elapsed: 0m1s\n",
      "2022-02-25 19:07:50.586857: Epoch: 19, Batch: 44, Loss: 0.5392, Elapsed: 0m1s\n",
      "2022-02-25 19:07:52.208270: Epoch: 19, Batch: 45, Loss: 0.5027, Elapsed: 0m1s\n",
      "2022-02-25 19:07:53.844203: Epoch: 19, Batch: 46, Loss: 0.5034, Elapsed: 0m1s\n",
      "2022-02-25 19:07:55.324279: Epoch: 19, Batch: 47, Loss: 0.5477, Elapsed: 0m1s\n",
      "2022-02-25 19:07:57.265223: Epoch: 19, Batch: 48, Loss: 0.4815, Elapsed: 0m1s\n",
      "2022-02-25 19:07:59.492681: Epoch: 19, Batch: 49, Loss: 0.4547, Elapsed: 0m2s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:08:00.907246: Epoch: 19, Batch: 50, Loss: 0.5519, Elapsed: 0m1s\n",
      "2022-02-25 19:08:01.122608 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 19:08:42.276091: validation Test:  Loss: 0.5027,  AUC: 0.6592, Acc: 77.9596,  Precision: 0.5840 -- Elapsed: 0m41s\n",
      "2022-02-25 19:08:42.276164 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 19:09:21.642853: training Test:  Loss: 0.5130,  AUC: 0.6560, Acc: 77.2412,  Precision: 0.6114 -- Elapsed: 0m39s\n",
      "2022-02-25 19:09:22.720871: Epoch: 20, Batch: 1, Loss: 0.5462, Elapsed: 0m1s\n",
      "2022-02-25 19:09:23.851348: Epoch: 20, Batch: 2, Loss: 0.5913, Elapsed: 0m0s\n",
      "2022-02-25 19:09:25.084378: Epoch: 20, Batch: 3, Loss: 0.5733, Elapsed: 0m1s\n",
      "2022-02-25 19:09:26.716941: Epoch: 20, Batch: 4, Loss: 0.5035, Elapsed: 0m1s\n",
      "2022-02-25 19:09:27.871032: Epoch: 20, Batch: 5, Loss: 0.5661, Elapsed: 0m0s\n",
      "2022-02-25 19:09:29.409105: Epoch: 20, Batch: 6, Loss: 0.5396, Elapsed: 0m1s\n",
      "2022-02-25 19:09:31.177002: Epoch: 20, Batch: 7, Loss: 0.5359, Elapsed: 0m1s\n",
      "2022-02-25 19:09:32.554894: Epoch: 20, Batch: 8, Loss: 0.5280, Elapsed: 0m1s\n",
      "2022-02-25 19:09:34.737365: Epoch: 20, Batch: 9, Loss: 0.4573, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "2022-02-25 19:09:37.104894: Epoch: 20, Batch: 10, Loss: 0.4509, Elapsed: 0m2s\n",
      "2022-02-25 19:09:39.085446: Epoch: 20, Batch: 11, Loss: 0.4850, Elapsed: 0m1s\n",
      "2022-02-25 19:09:40.578597: Epoch: 20, Batch: 12, Loss: 0.5343, Elapsed: 0m1s\n",
      "2022-02-25 19:09:42.116326: Epoch: 20, Batch: 13, Loss: 0.5239, Elapsed: 0m1s\n",
      "2022-02-25 19:09:44.377475: Epoch: 20, Batch: 14, Loss: 0.4232, Elapsed: 0m2s\n",
      "2022-02-25 19:09:46.722135: Epoch: 20, Batch: 15, Loss: 0.4921, Elapsed: 0m2s\n",
      "2022-02-25 19:09:48.390228: Epoch: 20, Batch: 16, Loss: 0.5063, Elapsed: 0m1s\n",
      "2022-02-25 19:09:49.855019: Epoch: 20, Batch: 17, Loss: 0.5419, Elapsed: 0m1s\n",
      "2022-02-25 19:09:51.601696: Epoch: 20, Batch: 18, Loss: 0.5136, Elapsed: 0m1s\n",
      "2022-02-25 19:09:52.910556: Epoch: 20, Batch: 19, Loss: 0.5413, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:09:54.771200: Epoch: 20, Batch: 20, Loss: 0.5166, Elapsed: 0m1s\n",
      "2022-02-25 19:09:56.256593: Epoch: 20, Batch: 21, Loss: 0.4900, Elapsed: 0m1s\n",
      "2022-02-25 19:09:57.930666: Epoch: 20, Batch: 22, Loss: 0.5307, Elapsed: 0m1s\n",
      "2022-02-25 19:09:59.620128: Epoch: 20, Batch: 23, Loss: 0.5008, Elapsed: 0m1s\n",
      "2022-02-25 19:10:01.713607: Epoch: 20, Batch: 24, Loss: 0.4905, Elapsed: 0m1s\n",
      "2022-02-25 19:10:03.673031: Epoch: 20, Batch: 25, Loss: 0.4932, Elapsed: 0m1s\n",
      "2022-02-25 19:10:05.049137: Epoch: 20, Batch: 26, Loss: 0.5407, Elapsed: 0m1s\n",
      "2022-02-25 19:10:06.423759: Epoch: 20, Batch: 27, Loss: 0.5533, Elapsed: 0m1s\n",
      "2022-02-25 19:10:07.639015: Epoch: 20, Batch: 28, Loss: 0.5542, Elapsed: 0m1s\n",
      "2022-02-25 19:10:10.188833: Epoch: 20, Batch: 29, Loss: 0.4639, Elapsed: 0m2s\n",
      "2022-02-25 19:10:11.939981: Epoch: 20, Batch: 30, Loss: 0.5282, Elapsed: 0m1s\n",
      "2022-02-25 19:10:13.587178: Epoch: 20, Batch: 31, Loss: 0.5006, Elapsed: 0m1s\n",
      "2022-02-25 19:10:15.375868: Epoch: 20, Batch: 32, Loss: 0.4810, Elapsed: 0m1s\n",
      "2022-02-25 19:10:16.643648: Epoch: 20, Batch: 33, Loss: 0.5310, Elapsed: 0m1s\n",
      "2022-02-25 19:10:18.052957: Epoch: 20, Batch: 34, Loss: 0.5505, Elapsed: 0m1s\n",
      "2022-02-25 19:10:19.878294: Epoch: 20, Batch: 35, Loss: 0.5196, Elapsed: 0m1s\n",
      "2022-02-25 19:10:21.328778: Epoch: 20, Batch: 36, Loss: 0.4777, Elapsed: 0m1s\n",
      "2022-02-25 19:10:22.361713: Epoch: 20, Batch: 37, Loss: 0.5898, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:10:23.472514: Epoch: 20, Batch: 38, Loss: 0.5554, Elapsed: 0m0s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "2022-02-25 19:10:24.946232: Epoch: 20, Batch: 39, Loss: 0.5482, Elapsed: 0m1s\n",
      "2022-02-25 19:10:26.389871: Epoch: 20, Batch: 40, Loss: 0.5402, Elapsed: 0m1s\n",
      "2022-02-25 19:10:27.939997: Epoch: 20, Batch: 41, Loss: 0.5306, Elapsed: 0m1s\n",
      "2022-02-25 19:10:29.430466: Epoch: 20, Batch: 42, Loss: 0.5602, Elapsed: 0m1s\n",
      "2022-02-25 19:10:30.956957: Epoch: 20, Batch: 43, Loss: 0.5329, Elapsed: 0m1s\n",
      "2022-02-25 19:10:32.968740: Epoch: 20, Batch: 44, Loss: 0.5079, Elapsed: 0m1s\n",
      "2022-02-25 19:10:34.316716: Epoch: 20, Batch: 45, Loss: 0.5488, Elapsed: 0m1s\n",
      "Vanishing gradients (layer 0 - GNN/InputNet/kernel:0): 2 out of 12 (16.666666666666668%)\n",
      "Vanishing gradients (layer 1 - GNN/InputNet/bias:0): 1 out of 4 (25.0%)\n",
      "Vanishing gradients (layer 7 - Variable:0): 2 out of 16 (12.5%)\n",
      "2022-02-25 19:10:35.851838: Epoch: 20, Batch: 46, Loss: 0.5143, Elapsed: 0m1s\n",
      "2022-02-25 19:10:37.558929: Epoch: 20, Batch: 47, Loss: 0.4947, Elapsed: 0m1s\n",
      "2022-02-25 19:10:39.100537: Epoch: 20, Batch: 48, Loss: 0.5516, Elapsed: 0m1s\n",
      "2022-02-25 19:10:41.061794: Epoch: 20, Batch: 49, Loss: 0.5031, Elapsed: 0m1s\n",
      "2022-02-25 19:10:42.174671: Epoch: 20, Batch: 50, Loss: 0.5597, Elapsed: 0m0s\n",
      "2022-02-25 19:10:42.388577 Starting testing the valid set with 50 subgraphs!\n",
      "2022-02-25 19:11:24.159792: validation Test:  Loss: 0.4992,  AUC: 0.6630, Acc: 78.3987,  Precision: 0.7121 -- Elapsed: 0m41s\n",
      "2022-02-25 19:11:24.159865 Starting testing the train set with 50 subgraphs!\n",
      "2022-02-25 19:12:03.992344: training Test:  Loss: 0.5106,  AUC: 0.6583, Acc: 77.5689,  Precision: 0.7381 -- Elapsed: 0m39s\n",
      "2022-02-25 19:12:03.992421: Training completed!\n"
     ]
    }
   ],
   "source": [
    "!python3 train.py configs/test_QGNN_vanburen.yaml 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbda33c6-ae3e-45c5-ae7c-22690d738b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!time python -m cProfile -s cumtime train.py configs/spsa_config.yaml 1 > profile.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bc622c6-a234-4753-99bc-8739b2410c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/qtrkx-gnn-tracking'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8518e94-e3fd-4c2c-9d61-fb7c2dbf6049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/qtrkx-gnn-tracking/configs\n"
     ]
    }
   ],
   "source": [
    "cd configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "897257fa-d961-45e2-8f3d-82a7364ef540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/qtrkx-gnn-tracking\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b942a-d60b-48f2-8c55-d2ca27b5f692",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
